{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "from pyannote.audio import Pipeline\n",
    "import shutil\n",
    "\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)\n",
    "asr_logger = logging.getLogger(\"asr\")\n",
    "asr_logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dia_model_name = \"pyannote/speaker-diarization@2022.07\"\n",
    "asr_model_name = \"stt_en_conformer_ctc_small\"  #'QuartzNet15x5Base-En'\n",
    "punct_model_name = \"punctuation_en_bert\"\n",
    "\n",
    "dia_model = Pipeline.from_pretrained(dia_model_name)\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=asr_model_name)\n",
    "punct_model = punct_model = PunctuationCapitalizationModel.from_pretrained(punct_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "pause_threshold = 1  # RE: collapsing diarised segments\n",
    "batch_size = 4\n",
    "offset = -0.18  # calibration offset for timestamps: 180 ms\n",
    "\n",
    "# load models\n",
    "vocab = asr_model.decoder.vocabulary\n",
    "vocab.append(\"_\")\n",
    "decoder = CTCBeamDecoder(\n",
    "    vocab,\n",
    "    beam_width=1,\n",
    "    blank_id=vocab.index(\"_\"),\n",
    "    log_probs_input=True,\n",
    ")\n",
    "time_stride = 1 / asr_model.cfg.preprocessor.window_size  # duration of model timesteps\n",
    "\n",
    "# from_disk(model_path)\n",
    "time_pad = 1\n",
    "# huge possible max audio if model is Quartznet; maximise where possible to limit segmentation transcription error\n",
    "second_max_audio = 120 if asr_model_name == \"QuartzNet15x5Base-En\" else 4\n",
    "round_value = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample_normalize_audio(in_file, out_file, sample_rate=16000):\n",
    "    # upsample/normalize audio to 16khz WAV\n",
    "    # via https://github.com/NVIDIA/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb\n",
    "    if not os.path.exists(in_file):\n",
    "        raise ValueError(f\"{in_file} not found\")\n",
    "    if out_file is None:\n",
    "        out_file = in_file.replace(os.path.splitext(in_file)[-1], f\"_{sample_rate}.wav\")\n",
    "\n",
    "    os.system(\n",
    "        f\"ffmpeg -i {in_file} -acodec pcm_s16le -ac 1 -af aresample=resampler=soxr -ar {sample_rate} {out_file} -y\"\n",
    "    )\n",
    "    return out_file\n",
    "\n",
    "def _split_stereo_audio(in_file, out_dir):\n",
    "    # into left/right channel wavs\n",
    "    in_file = Path(in_file)\n",
    "    out_dir = Path(out_dir)\n",
    "    assert in_file.exists()\n",
    "    assert out_dir.exists()\n",
    "\n",
    "    # format output files\n",
    "    left_channel = out_dir / f\"{Path(in_file).stem}_left.wav\"\n",
    "    right_channel = out_dir / f\"{Path(in_file).stem}_right.wav\"\n",
    "\n",
    "    # split, export\n",
    "    audio_segment = AudioSegment.from_file(in_file)\n",
    "    monos = audio_segment.split_to_mono()\n",
    "    assert len(monos) == 2  # cap support at stereo audio\n",
    "    monos[0].export(left_channel, format=\"wav\")\n",
    "    monos[1].export(right_channel, format=\"wav\")\n",
    "\n",
    "    return left_channel, right_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diarize_mono_audio(in_file):\n",
    "    diarization_raw = dia_model(str(in_file))\n",
    "    diarized_segments = (\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n",
    "                for turn, _, speaker in diarization_raw.itertracks(yield_label=True)\n",
    "            ]\n",
    "        )\n",
    "        # shift speaker attribution > mark/collapse consecutive speaker segments\n",
    "        .assign(segment_marker=lambda x: x.speaker.shift(1)).assign(\n",
    "            segment_marker=lambda x: x.segment_marker != x.speaker\n",
    "        )\n",
    "        .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker))\n",
    "    )\n",
    "\n",
    "    diarized_segments = (\n",
    "        diarized_segments\n",
    "        # groupby/aggregate shifted, collapse consecutive speaker sequences\n",
    "        .groupby(\"segment_marker\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"speaker\": \"first\",\n",
    "                \"start\": \"first\",\n",
    "                \"end\": \"last\",\n",
    "                \"segment_marker\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .rename(\n",
    "            mapper={\"segment_marker\": \"segment_marker_count\"},\n",
    "            axis=\"columns\",\n",
    "            inplace=False,\n",
    "        )\n",
    "        .assign(segment_len=lambda x: x.end - x.start)\n",
    "        # reconcile very short segments with pre/proceeding segment? merging strategy?\n",
    "        #             .query('segment_len >= 1')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return diarized_segments\n",
    "\n",
    "x = _diarize_mono_audio('../output/temp_dir/methadone-stigma.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>segment_marker_count</th>\n",
       "      <th>segment_len</th>\n",
       "      <th>start_min</th>\n",
       "      <th>end_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>71.254688</td>\n",
       "      <td>74.950312</td>\n",
       "      <td>3</td>\n",
       "      <td>3.695625</td>\n",
       "      <td>1.187578</td>\n",
       "      <td>1.249172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>212.346563</td>\n",
       "      <td>212.633438</td>\n",
       "      <td>1</td>\n",
       "      <td>0.286875</td>\n",
       "      <td>3.539109</td>\n",
       "      <td>3.543891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>212.633438</td>\n",
       "      <td>215.215312</td>\n",
       "      <td>2</td>\n",
       "      <td>2.581875</td>\n",
       "      <td>3.543891</td>\n",
       "      <td>3.586922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>264.237188</td>\n",
       "      <td>269.114063</td>\n",
       "      <td>1</td>\n",
       "      <td>4.876875</td>\n",
       "      <td>4.403953</td>\n",
       "      <td>4.485234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>415.403438</td>\n",
       "      <td>420.027188</td>\n",
       "      <td>2</td>\n",
       "      <td>4.623750</td>\n",
       "      <td>6.923391</td>\n",
       "      <td>7.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>495.441563</td>\n",
       "      <td>495.492188</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>8.257359</td>\n",
       "      <td>8.258203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>564.764063</td>\n",
       "      <td>565.641563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>9.412734</td>\n",
       "      <td>9.427359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       speaker       start         end  segment_marker_count  segment_len  \\\n",
       "2   SPEAKER_01   71.254688   74.950312                     3     3.695625   \n",
       "5   SPEAKER_00  212.346563  212.633438                     1     0.286875   \n",
       "6   SPEAKER_01  212.633438  215.215312                     2     2.581875   \n",
       "8   SPEAKER_01  264.237188  269.114063                     1     4.876875   \n",
       "14  SPEAKER_01  415.403438  420.027188                     2     4.623750   \n",
       "18  SPEAKER_02  495.441563  495.492188                     1     0.050625   \n",
       "23  SPEAKER_00  564.764063  565.641563                     1     0.877500   \n",
       "\n",
       "    start_min   end_min  \n",
       "2    1.187578  1.249172  \n",
       "5    3.539109  3.543891  \n",
       "6    3.543891  3.586922  \n",
       "8    4.403953  4.485234  \n",
       "14   6.923391  7.000453  \n",
       "18   8.257359  8.258203  \n",
       "23   9.412734  9.427359  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what to do with these tiny segments?\n",
    "(x\n",
    ".assign(start_min=lambda x: x.start/60)\n",
    ".assign(end_min=lambda x: x.end/60)\n",
    ".query('segment_len < 5')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_word_timestamps(asr_output, chunk_offset):\n",
    "    preds = asr_output.y_sequence.tolist()  # some funky formatting\n",
    "    probs_seq = torch.FloatTensor([preds])  # some funky formatting\n",
    "    beam_result, beam_scores, timesteps, out_seq_len = decoder.decode(probs_seq)\n",
    "    lens = out_seq_len[0][0]\n",
    "    timesteps = timesteps[0][0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    if len(timesteps) == 0:\n",
    "        return result\n",
    "\n",
    "    start = (timesteps[0] - TIME_PAD) * time_stride + chunk_offset\n",
    "    end = (timesteps[0] + TIME_PAD * 2) * time_stride + chunk_offset\n",
    "\n",
    "    token_prev = vocab[int(beam_result[0][0][0])]\n",
    "    word = token_prev\n",
    "\n",
    "    for n in range(1, lens):\n",
    "        token = vocab[int(beam_result[0][0][n])]\n",
    "\n",
    "        if token[0] == \"#\":\n",
    "            # merge subwords\n",
    "            word = word + token[2:]\n",
    "\n",
    "        elif token[0] == \"-\" or token_prev[0] == \"-\":\n",
    "            word = word + token\n",
    "\n",
    "        else:\n",
    "            word = word.replace(\"▁\", \"\").replace(\"_\", \"\")  # remove weird token\n",
    "\n",
    "            result_word = {\n",
    "                \"startTime\": int(start) / 1000,\n",
    "                \"endTime\": int(end) / 1000,\n",
    "                \"word\": word,\n",
    "            }\n",
    "            result.append(result_word)\n",
    "\n",
    "            start = (timesteps[n] - TIME_PAD) * time_stride + chunk_offset\n",
    "            word = token\n",
    "\n",
    "        end = (timesteps[n] + TIME_PAD * 2) * time_stride + chunk_offset\n",
    "        token_prev = token\n",
    "\n",
    "    # add last word\n",
    "    word = word.replace(\"▁\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "    result_word = {\n",
    "        \"startTime\": int(start) / 1000,\n",
    "        \"endTime\": int(end) / 1000,\n",
    "        \"word\": word,\n",
    "    }\n",
    "    result.append(result_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _gcp_format_single_utterance(time_formatted_words_single, channel_tag=None):\n",
    "#     # GCP STT consistency etc. why are we standardising on this again?\n",
    "#     string_formatted_word_stamps = []\n",
    "#     for e in time_formatted_words_single:\n",
    "#         temp = deepcopy(e)\n",
    "#         temp[\"startTime\"] = f\"{e['startTime']}s\"\n",
    "#         temp[\"endTime\"] = f\"{e['endTime']}s\"\n",
    "#         string_formatted_word_stamps.append(temp)\n",
    "\n",
    "#     return {\n",
    "#         \"alternatives\": [\n",
    "#             {\n",
    "#                 \"transcript\": \" \".join(e[\"word\"] for e in time_formatted_words_single),\n",
    "#                 \"words\": string_formatted_word_stamps,\n",
    "#             }\n",
    "#         ],\n",
    "#         \"speakerTag\": time_formatted_words_single[0][\"speakerTag\"],\n",
    "#         \"channelTag\": \"None\" if not channel_tag else channel_tag,\n",
    "#         \"languageCode\": \"en\",\n",
    "#     }\n",
    "\n",
    "\n",
    "# def _gcp_format_aggregate_transcript(time_formatted_words_all):\n",
    "#     # GCP STT consistency etc.\n",
    "#     transcript_all = [\n",
    "#         \" \".join(e[\"word\"] for e in segment_transcript)\n",
    "#         for segment_transcript in time_formatted_words_all\n",
    "#     ]\n",
    "\n",
    "#     return \" \".join(transcript_all)\n",
    "\n",
    "\n",
    "# def _gcp_format_channel_seperated_transcript_objects(\n",
    "#     gcp_formatted_left_res, gcp_formatted_right_res\n",
    "# ):\n",
    "#     # merge, sort individual left/right transcripts\n",
    "#     merged_utterances = pd.concat(\n",
    "#         [\n",
    "#             format_utterances_df(gcp_formatted_left_res),\n",
    "#             format_utterances_df(gcp_formatted_right_res),\n",
    "#         ]\n",
    "#     ).sort_values(by=[\"startTime\", \"endTime\"])\n",
    "\n",
    "#     # use any/left metadata as base (should be the same file right?)\n",
    "#     merged_metadata = {\n",
    "#         k: v for k, v in gcp_formatted_left_res[\"metadata\"].items() if k != \"transcript\"\n",
    "#     }\n",
    "#     merged_transcript = \" \".join(merged_utterances.transcript.tolist())\n",
    "#     merged_metadata[\"transcript\"] = merged_transcript\n",
    "\n",
    "#     return {\n",
    "#         \"metadata\": merged_metadata,\n",
    "#         \"streaming_outputs\": (\n",
    "#             merged_utterances.pipe(\n",
    "#                 lambda x: x[\n",
    "#                     [\n",
    "#                         \"alternatives\",\n",
    "#                         \"speakerTag\",\n",
    "#                         \"channelTag\",\n",
    "#                         \"languageCode\",\n",
    "#                     ]\n",
    "#                 ]\n",
    "#             ).to_dict(orient=\"records\")\n",
    "#         ),\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _naively_segment_utterances(record):\n",
    "    # apply naive splitting\n",
    "    n_chunks = int((record.end - record.start) // second_max_audio) + 1\n",
    "    chunk_len = (record.end - record.start) / n_chunks\n",
    "\n",
    "    df_temp = pd.DataFrame([record] * n_chunks).reset_index(drop=True)\n",
    "    df_temp[\"start\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len * x.name, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp[\"end\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp.loc[\n",
    "        (n_chunks - 1), \"end\"\n",
    "    ] = (\n",
    "        record.end\n",
    "    )  # adjust end time to actual time (sanity correction in case rounding cuts of audio)\n",
    "    return df_temp.assign(segment_len=lambda x: x.end - x.start)\n",
    "\n",
    "\n",
    "def _segment_utterances(audio_segment, record):\n",
    "    dBFS = audio_segment.dBFS  # audio volume (silence level is relative to volume)\n",
    "    silences = detect_silence(\n",
    "        audio_segment, min_silence_len=500, silence_thresh=dBFS - 20\n",
    "    )  # 0.5 break, time in ms, silence_thresh 20 lower than audio volume\n",
    "\n",
    "    if len(silences) == 0:\n",
    "        # no silence detected, lower min_silence_len\n",
    "        silences = detect_silence(\n",
    "            audio_segment, min_silence_len=100, silence_thresh=dBFS - 20\n",
    "        )\n",
    "\n",
    "        if len(silences) == 0:\n",
    "            # if still no silences detected after lowering min_silence_len, split naively\n",
    "            return _naively_segment_utterances(record)\n",
    "\n",
    "    silences = [[(s[1] - s[0]), s[0] / 1000, s[1] / 1000] for s in silences]  # ms -> s\n",
    "\n",
    "    df_temp = pd.DataFrame(record).T.reset_index(drop=True)\n",
    "\n",
    "    # split on longest silence, in middle of silence so no info is lost\n",
    "    while (len(silences) > 0) & any(df_temp.segment_len > second_max_audio):\n",
    "        longest_silence = silences.pop(silences.index(max(silences)))\n",
    "        middle_silence = record.start + (\n",
    "            longest_silence[1] + (longest_silence[2] - longest_silence[1]) / 2\n",
    "        )\n",
    "\n",
    "        record_to_split = df_temp.query(\n",
    "            f\"start < {middle_silence} & end>{middle_silence} & segment_len > {second_max_audio}\"\n",
    "        )\n",
    "        df_temp = df_temp.drop(record_to_split.index)\n",
    "\n",
    "        split_utterances = pd.DataFrame(\n",
    "            [record_to_split.iloc[0], record_to_split.iloc[0]]\n",
    "        ).reset_index(drop=True)\n",
    "        split_utterances.loc[0, \"end\"] = middle_silence\n",
    "        split_utterances.loc[1, \"start\"] = middle_silence\n",
    "        df_temp = (\n",
    "            pd.concat([df_temp, split_utterances])\n",
    "            .reset_index(drop=True)\n",
    "            .assign(segment_len=lambda x: x.end - x.start)\n",
    "        )\n",
    "\n",
    "    if any(df_temp.segment_len > second_max_audio):\n",
    "        # if any segments are still too long, naively split them\n",
    "        final_df = [df_temp.query(f\"segment_len < {second_max_audio}\")]\n",
    "        records_to_split = df_temp.query(f\"segment_len > {second_max_audio}\")\n",
    "\n",
    "        for i, record in records_to_split.iterrows():\n",
    "            final_df.append(_naively_segment_utterances(record))\n",
    "        return pd.concat(final_df).reset_index(drop=True).sort_values(by=[\"start\"])\n",
    "\n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mp3, from '../output/radio_national_podcasts/audio/methadone-stigma.mp3':\n",
      "  Duration: 00:09:44.98, start: 0.023021, bitrate: 128 kb/s\n",
      "    Stream #0:0: Audio: mp3, 48000 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : LAME3.98r\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '../output/temp_dir/methadone-stigma.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.29.100\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 pcm_s16le\n",
      "size=   18279kB time=00:09:44.93 bitrate= 256.0kbits/s speed= 527x    \n",
      "video:0kB audio:18279kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000417%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blog-os-asr/notebooks/2_asr.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtempfile\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000013vscode-remote?line=4'>5</a>\u001b[0m _transcribe_mono(\u001b[39m'\u001b[39;49m\u001b[39m../output/radio_national_podcasts/audio/methadone-stigma.mp3\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/blog-os-asr/notebooks/2_asr.ipynb Cell 10'\u001b[0m in \u001b[0;36m_transcribe_mono\u001b[0;34m(input_file, single_speaker)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000010vscode-remote?line=15'>16</a>\u001b[0m asr_logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mSuccessfully resampled/converted input to WAV\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000010vscode-remote?line=17'>18</a>\u001b[0m \u001b[39m# 2.0 diarize input, save diarised segments\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000010vscode-remote?line=18'>19</a>\u001b[0m diarized_segments \u001b[39m=\u001b[39m _diarize_mono_audio(wav_path, single_speaker)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000010vscode-remote?line=19'>20</a>\u001b[0m paths2audio_files \u001b[39m=\u001b[39m []  \u001b[39m# explicitly sequence, RE: sorted() issues\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000010vscode-remote?line=21'>22</a>\u001b[0m chunked_diarized_segments \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/home/blog-os-asr/notebooks/2_asr.ipynb Cell 5'\u001b[0m in \u001b[0;36m_diarize_mono_audio\u001b[0;34m(in_file, single_speaker)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=25'>26</a>\u001b[0m     \u001b[39m# split diarized segments on predicted speaker attribution (less risky? - original intent of model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=26'>27</a>\u001b[0m     \u001b[39m# eg. in the case of a \"mashed\" single audio file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=27'>28</a>\u001b[0m     diarized_segments \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=28'>29</a>\u001b[0m         \u001b[39m# format diarization as a dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=29'>30</a>\u001b[0m         pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=40'>41</a>\u001b[0m         \u001b[39m.\u001b[39massign(segment_marker\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: pd\u001b[39m.\u001b[39mSeries\u001b[39m.\u001b[39mcumsum(x\u001b[39m.\u001b[39msegment_marker))\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=41'>42</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=43'>44</a>\u001b[0m diarized_segments \u001b[39m=\u001b[39m (\n\u001b[0;32m---> <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=44'>45</a>\u001b[0m     diarized_segments\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=45'>46</a>\u001b[0m     \u001b[39m# groupby/aggregate shifted, collapse consecutive speaker sequences\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39m.\u001b[39;49mgroupby(\u001b[39m\"\u001b[39;49m\u001b[39msegment_marker\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=47'>48</a>\u001b[0m     \u001b[39m.\u001b[39;49magg(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=48'>49</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=49'>50</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mspeaker\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mfirst\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=50'>51</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstart\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mfirst\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=51'>52</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mlast\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=52'>53</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39msegment_marker\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mcount\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=53'>54</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=54'>55</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=55'>56</a>\u001b[0m     \u001b[39m.\u001b[39;49mrename(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=56'>57</a>\u001b[0m         mapper\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39msegment_marker\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msegment_marker_count\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=57'>58</a>\u001b[0m         axis\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=58'>59</a>\u001b[0m         inplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=59'>60</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=60'>61</a>\u001b[0m     \u001b[39m.\u001b[39;49massign(segment_len\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mend \u001b[39m-\u001b[39;49m x\u001b[39m.\u001b[39;49mstart)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=61'>62</a>\u001b[0m     \u001b[39m# reconcile very short segments with pre/proceeding segment? merging strategy?\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=62'>63</a>\u001b[0m     \u001b[39m#             .query('segment_len >= 1')\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=63'>64</a>\u001b[0m     \u001b[39m# remap speaker tags from letters to numbers\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=64'>65</a>\u001b[0m     \u001b[39m.\u001b[39;49mpipe(\u001b[39mlambda\u001b[39;49;00m x: x[x\u001b[39m.\u001b[39;49mspeaker\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m y: \u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mtype\u001b[39;49m(y) \u001b[39m==\u001b[39;49m \u001b[39mstr\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m)])\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=65'>66</a>\u001b[0m     \u001b[39m.\u001b[39;49massign(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=66'>67</a>\u001b[0m         speaker\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mspeaker\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=67'>68</a>\u001b[0m             \u001b[39mlambda\u001b[39;49;00m y: \u001b[39mint\u001b[39;49m(speaker_tag_remap\u001b[39m.\u001b[39;49mget(y\u001b[39m.\u001b[39;49mlower()))\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=68'>69</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=69'>70</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=70'>71</a>\u001b[0m     \u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=71'>72</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=73'>74</a>\u001b[0m \u001b[39mreturn\u001b[39;00m diarized_segments\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/frame.py:4512\u001b[0m, in \u001b[0;36mDataFrame.assign\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/frame.py?line=4508'>4509</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/frame.py?line=4510'>4511</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m-> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/frame.py?line=4511'>4512</a>\u001b[0m     data[k] \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39;49mapply_if_callable(v, data)\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/frame.py?line=4512'>4513</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py:358\u001b[0m, in \u001b[0;36mapply_if_callable\u001b[0;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=346'>347</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=347'>348</a>\u001b[0m \u001b[39mEvaluate possibly callable input using obj and kwargs if it is callable,\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=348'>349</a>\u001b[0m \u001b[39motherwise return as it is.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=354'>355</a>\u001b[0m \u001b[39m**kwargs\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=355'>356</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=356'>357</a>\u001b[0m \u001b[39mif\u001b[39;00m callable(maybe_callable):\n\u001b[0;32m--> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=357'>358</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m maybe_callable(obj, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/common.py?line=359'>360</a>\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_callable\n",
      "\u001b[1;32m/home/blog-os-asr/notebooks/2_asr.ipynb Cell 5'\u001b[0m in \u001b[0;36m_diarize_mono_audio.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=25'>26</a>\u001b[0m     \u001b[39m# split diarized segments on predicted speaker attribution (less risky? - original intent of model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=26'>27</a>\u001b[0m     \u001b[39m# eg. in the case of a \"mashed\" single audio file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=27'>28</a>\u001b[0m     diarized_segments \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=28'>29</a>\u001b[0m         \u001b[39m# format diarization as a dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=29'>30</a>\u001b[0m         pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=40'>41</a>\u001b[0m         \u001b[39m.\u001b[39massign(segment_marker\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: pd\u001b[39m.\u001b[39mSeries\u001b[39m.\u001b[39mcumsum(x\u001b[39m.\u001b[39msegment_marker))\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=41'>42</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=43'>44</a>\u001b[0m diarized_segments \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=44'>45</a>\u001b[0m     diarized_segments\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=45'>46</a>\u001b[0m     \u001b[39m# groupby/aggregate shifted, collapse consecutive speaker sequences\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39m.\u001b[39mgroupby(\u001b[39m\"\u001b[39m\u001b[39msegment_marker\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=47'>48</a>\u001b[0m     \u001b[39m.\u001b[39magg(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=48'>49</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=49'>50</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mspeaker\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=50'>51</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=51'>52</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=52'>53</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msegment_marker\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=53'>54</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=54'>55</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=55'>56</a>\u001b[0m     \u001b[39m.\u001b[39mrename(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=56'>57</a>\u001b[0m         mapper\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39msegment_marker\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msegment_marker_count\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=57'>58</a>\u001b[0m         axis\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=58'>59</a>\u001b[0m         inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=59'>60</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=60'>61</a>\u001b[0m     \u001b[39m.\u001b[39massign(segment_len\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mend \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mstart)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=61'>62</a>\u001b[0m     \u001b[39m# reconcile very short segments with pre/proceeding segment? merging strategy?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=62'>63</a>\u001b[0m     \u001b[39m#             .query('segment_len >= 1')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=63'>64</a>\u001b[0m     \u001b[39m# remap speaker tags from letters to numbers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=64'>65</a>\u001b[0m     \u001b[39m.\u001b[39mpipe(\u001b[39mlambda\u001b[39;00m x: x[x\u001b[39m.\u001b[39mspeaker\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m y: \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(y) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m)])\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=65'>66</a>\u001b[0m     \u001b[39m.\u001b[39massign(\n\u001b[0;32m---> <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=66'>67</a>\u001b[0m         speaker\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39;49mspeaker\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=67'>68</a>\u001b[0m             \u001b[39mlambda\u001b[39;49;00m y: \u001b[39mint\u001b[39;49m(speaker_tag_remap\u001b[39m.\u001b[39;49mget(y\u001b[39m.\u001b[39;49mlower()))\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=68'>69</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=69'>70</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=70'>71</a>\u001b[0m     \u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=71'>72</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=73'>74</a>\u001b[0m \u001b[39mreturn\u001b[39;00m diarized_segments\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4322'>4323</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4323'>4324</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4329'>4330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4330'>4331</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4331'>4332</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4430'>4431</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4431'>4432</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4432'>4433</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1083'>1084</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1084'>1085</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1085'>1086</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1087'>1088</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1141'>1142</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m             values,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1146'>1147</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1148'>1149</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1149'>1150</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1150'>1151</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1151'>1152</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/home/blog-os-asr/notebooks/2_asr.ipynb Cell 5'\u001b[0m in \u001b[0;36m_diarize_mono_audio.<locals>.<lambda>.<locals>.<lambda>\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=25'>26</a>\u001b[0m     \u001b[39m# split diarized segments on predicted speaker attribution (less risky? - original intent of model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=26'>27</a>\u001b[0m     \u001b[39m# eg. in the case of a \"mashed\" single audio file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=27'>28</a>\u001b[0m     diarized_segments \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=28'>29</a>\u001b[0m         \u001b[39m# format diarization as a dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=29'>30</a>\u001b[0m         pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=40'>41</a>\u001b[0m         \u001b[39m.\u001b[39massign(segment_marker\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: pd\u001b[39m.\u001b[39mSeries\u001b[39m.\u001b[39mcumsum(x\u001b[39m.\u001b[39msegment_marker))\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=41'>42</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=43'>44</a>\u001b[0m diarized_segments \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=44'>45</a>\u001b[0m     diarized_segments\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=45'>46</a>\u001b[0m     \u001b[39m# groupby/aggregate shifted, collapse consecutive speaker sequences\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39m.\u001b[39mgroupby(\u001b[39m\"\u001b[39m\u001b[39msegment_marker\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=47'>48</a>\u001b[0m     \u001b[39m.\u001b[39magg(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=48'>49</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=49'>50</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mspeaker\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=50'>51</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=51'>52</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=52'>53</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msegment_marker\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=53'>54</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=54'>55</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=55'>56</a>\u001b[0m     \u001b[39m.\u001b[39mrename(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=56'>57</a>\u001b[0m         mapper\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39msegment_marker\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msegment_marker_count\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=57'>58</a>\u001b[0m         axis\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=58'>59</a>\u001b[0m         inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=59'>60</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=60'>61</a>\u001b[0m     \u001b[39m.\u001b[39massign(segment_len\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mend \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mstart)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=61'>62</a>\u001b[0m     \u001b[39m# reconcile very short segments with pre/proceeding segment? merging strategy?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=62'>63</a>\u001b[0m     \u001b[39m#             .query('segment_len >= 1')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=63'>64</a>\u001b[0m     \u001b[39m# remap speaker tags from letters to numbers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=64'>65</a>\u001b[0m     \u001b[39m.\u001b[39mpipe(\u001b[39mlambda\u001b[39;00m x: x[x\u001b[39m.\u001b[39mspeaker\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m y: \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(y) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m)])\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=65'>66</a>\u001b[0m     \u001b[39m.\u001b[39massign(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=66'>67</a>\u001b[0m         speaker\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mspeaker\u001b[39m.\u001b[39mapply(\n\u001b[0;32m---> <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=67'>68</a>\u001b[0m             \u001b[39mlambda\u001b[39;00m y: \u001b[39mint\u001b[39;49m(speaker_tag_remap\u001b[39m.\u001b[39;49mget(y\u001b[39m.\u001b[39;49mlower()))\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=68'>69</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=69'>70</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=70'>71</a>\u001b[0m     \u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=71'>72</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=73'>74</a>\u001b[0m \u001b[39mreturn\u001b[39;00m diarized_segments\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "import string\n",
    "\n",
    "_transcribe_mono('../output/radio_national_podcasts/audio/methadone-stigma.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transcribe_mono(input_file, single_speaker=False):\n",
    "    # transcribe a mono wav file\n",
    "    input_file = Path(input_file)\n",
    "    asr_logger.info(f\"Transcribing: {input_file}..\")\n",
    "\n",
    "    temp_dir = Path('../output/temp_dir')\n",
    "    shutil.rmtree(str(temp_dir)) if temp_dir.exists() else None\n",
    "    temp_dir.mkdir(parents=True)\n",
    "\n",
    "    # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # 1.0 resample, convert to wav\n",
    "    wav_path = _resample_normalize_audio(\n",
    "        input_file, str(Path(temp_dir) / f\"{Path(input_file).stem}.wav\")\n",
    "    )\n",
    "    audio_segment = AudioSegment.from_file(wav_path)\n",
    "    asr_logger.info('Successfully resampled/converted input to WAV')\n",
    "\n",
    "    # 2.0 diarize input, save diarised segments\n",
    "    diarized_segments = _diarize_mono_audio(wav_path, single_speaker)\n",
    "    paths2audio_files = []  # explicitly sequence, RE: sorted() issues\n",
    "\n",
    "    chunked_diarized_segments = []\n",
    "    for idx, record in diarized_segments.iterrows():\n",
    "        if record.segment_len > second_max_audio:\n",
    "            records = _segment_utterances(\n",
    "                audio_segment[floor(record.start * 1000) : ceil(record.end * 1000)],\n",
    "                record,\n",
    "            )\n",
    "            chunked_diarized_segments.append(records)\n",
    "        else:\n",
    "            chunked_diarized_segments.append(\n",
    "                pd.DataFrame(record).T.reset_index(drop=True)\n",
    "            )\n",
    "    chunked_diarized_segments = pd.concat(chunked_diarized_segments).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    asr_logger.info('Successfully diarised input')\n",
    "\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        # slice audio per utterance, round start/end to floor/ceil inclusively\n",
    "        segment_audio = audio_segment[\n",
    "            floor(record.start * 1000) : ceil(record.end * 1000)\n",
    "        ]\n",
    "\n",
    "        # prevent misc output from printing\n",
    "        segment_audio_res = segment_audio.export(\n",
    "            Path(temp_dir) / f\"chunk_{idx}.wav\", format=\"wav\"\n",
    "        )\n",
    "        # collect segment audio path\n",
    "        paths2audio_files.append(str(Path(temp_dir) / f\"chunk_{idx}.wav\"))\n",
    "    asr_logger.info('Successfully chunked and saved diarised temp chunks')\n",
    "\n",
    "    # 3.0 batch transcribe, retrieve transcripts, alignments and logprobs for each utterance\n",
    "    outputs = asr_model.transcribe(\n",
    "        paths2audio_files=paths2audio_files,\n",
    "        batch_size=batch_size,\n",
    "        return_hypotheses=True,\n",
    "    )\n",
    "    asr_logger.info('Successfully processed chunks with ASR model')\n",
    "\n",
    "    # 4.0 retrieve/format timestamps\n",
    "    time_formatted_words_all = []\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        time_formatted_words = _format_word_timestamps(outputs[idx], record.start)\n",
    "\n",
    "        # 5.0 apply punctuation to each output\n",
    "        punctuated_sequence = punct_model.add_punctuation_capitalization(\n",
    "            [\" \".join(e[\"word\"] for e in time_formatted_words)]\n",
    "        )[0]\n",
    "\n",
    "        if len(punctuated_sequence.split(\" \")) == len(time_formatted_words):\n",
    "            # easy case, where punctuated output len matches input len; assign directly\n",
    "            punctuated_sequence_joined = (\n",
    "                pd.DataFrame(time_formatted_words)\n",
    "                .assign(word=punctuated_sequence.split(\" \"))\n",
    "                .assign(speakerTag=record.speaker)\n",
    "                .to_dict(orient=\"records\")\n",
    "            )\n",
    "            time_formatted_words_all.append(punctuated_sequence_joined)\n",
    "        else:\n",
    "            # otherwise.. pad the difference? changes should be limited to immediately proceeding fullstops, commas, question marks\n",
    "            # https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html\n",
    "            print(\"Punctuated outputs not the same length as input\")\n",
    "\n",
    "    return time_formatted_words_all\n",
    "\n",
    "\n",
    "def _transcribe_channel_seperated_audio(input_file):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # 1.0 split left/right channels\n",
    "        left_channel, right_channel = _split_stereo_audio(input_file, temp_dir)\n",
    "\n",
    "        # 2.0 process as seperate monos\n",
    "        left_res = _transcribe_mono(left_channel, single_speaker=True)\n",
    "        right_res = _transcribe_mono(right_channel, single_speaker=True)\n",
    "\n",
    "    # 3.0 merge outputs\n",
    "    return _gcp_format_channel_seperated_transcript_objects(left_res, right_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
