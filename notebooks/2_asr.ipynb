{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "from pyannote.audio import Pipeline\n",
    "import shutil\n",
    "\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)\n",
    "asr_logger = logging.getLogger(\"asr\")\n",
    "asr_logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dia_model_name = \"pyannote/speaker-diarization@2022.07\"\n",
    "asr_model_name = \"stt_en_conformer_ctc_small\"  #'QuartzNet15x5Base-En'\n",
    "punct_model_name = \"punctuation_en_bert\"\n",
    "\n",
    "dia_model = Pipeline.from_pretrained(dia_model_name)\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=asr_model_name)\n",
    "punct_model = punct_model = PunctuationCapitalizationModel.from_pretrained(punct_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "pause_threshold = 1  # RE: collapsing diarised segments\n",
    "batch_size = 4\n",
    "offset = -0.18  # calibration offset for timestamps: 180 ms\n",
    "\n",
    "# load models\n",
    "vocab = asr_model.decoder.vocabulary\n",
    "vocab.append(\"_\")\n",
    "decoder = CTCBeamDecoder(\n",
    "    vocab,\n",
    "    beam_width=1,\n",
    "    blank_id=vocab.index(\"_\"),\n",
    "    log_probs_input=True,\n",
    ")\n",
    "time_stride = 1 / asr_model.cfg.preprocessor.window_size  # duration of model timesteps\n",
    "\n",
    "# from_disk(model_path)\n",
    "time_pad = 1\n",
    "# huge possible max audio if model is Quartznet; maximise where possible to limit segmentation transcription error\n",
    "second_max_audio = 120 if asr_model_name == \"QuartzNet15x5Base-En\" else 4\n",
    "round_value = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample_normalize_audio(in_file, out_file, sample_rate=16000):\n",
    "    # upsample/normalize audio to 16khz WAV\n",
    "    # via https://github.com/NVIDIA/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb\n",
    "    if not os.path.exists(in_file):\n",
    "        raise ValueError(f\"{in_file} not found\")\n",
    "    if out_file is None:\n",
    "        out_file = in_file.replace(os.path.splitext(in_file)[-1], f\"_{sample_rate}.wav\")\n",
    "\n",
    "    os.system(\n",
    "        f\"ffmpeg -i {in_file} -acodec pcm_s16le -ac 1 -af aresample=resampler=soxr -ar {sample_rate} {out_file} -y\"\n",
    "    )\n",
    "    return out_file\n",
    "\n",
    "def _split_stereo_audio(in_file, out_dir):\n",
    "    # into left/right channel wavs\n",
    "    in_file = Path(in_file)\n",
    "    out_dir = Path(out_dir)\n",
    "    assert in_file.exists()\n",
    "    assert out_dir.exists()\n",
    "\n",
    "    # format output files\n",
    "    left_channel = out_dir / f\"{Path(in_file).stem}_left.wav\"\n",
    "    right_channel = out_dir / f\"{Path(in_file).stem}_right.wav\"\n",
    "\n",
    "    # split, export\n",
    "    audio_segment = AudioSegment.from_file(in_file)\n",
    "    monos = audio_segment.split_to_mono()\n",
    "    assert len(monos) == 2  # cap support at stereo audio\n",
    "    monos[0].export(left_channel, format=\"wav\")\n",
    "    monos[1].export(right_channel, format=\"wav\")\n",
    "\n",
    "    return left_channel, right_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diarize_mono_audio(in_file):\n",
    "    diarization_raw = dia_model(str(in_file))\n",
    "    diarized_segments = (\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n",
    "                for turn, _, speaker in diarization_raw.itertracks(yield_label=True)\n",
    "            ]\n",
    "        )\n",
    "        # shift speaker attribution > mark/collapse consecutive speaker segments\n",
    "        .assign(segment_marker=lambda x: x.speaker.shift(1)).assign(\n",
    "            segment_marker=lambda x: x.segment_marker != x.speaker\n",
    "        )\n",
    "        .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker))\n",
    "    )\n",
    "\n",
    "    diarized_segments = (\n",
    "        diarized_segments\n",
    "        # groupby/aggregate shifted, collapse consecutive speaker sequences\n",
    "        .groupby(\"segment_marker\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"speaker\": \"first\",\n",
    "                \"start\": \"first\",\n",
    "                \"end\": \"last\",\n",
    "                \"segment_marker\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .rename(\n",
    "            mapper={\"segment_marker\": \"segment_marker_count\"},\n",
    "            axis=\"columns\",\n",
    "            inplace=False,\n",
    "        )\n",
    "        .assign(segment_len=lambda x: x.end - x.start)\n",
    "        # reconcile very short segments with pre/proceeding segment? merging strategy?\n",
    "        #             .query('segment_len >= 1')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return diarized_segments\n",
    "\n",
    "x = _diarize_mono_audio('../output/temp_dir/methadone-stigma.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>segment_marker_count</th>\n",
       "      <th>segment_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>4.716563</td>\n",
       "      <td>22.283438</td>\n",
       "      <td>4</td>\n",
       "      <td>17.566875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>22.857187</td>\n",
       "      <td>71.052188</td>\n",
       "      <td>10</td>\n",
       "      <td>48.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>71.254688</td>\n",
       "      <td>74.950312</td>\n",
       "      <td>3</td>\n",
       "      <td>3.695625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>74.950312</td>\n",
       "      <td>198.914062</td>\n",
       "      <td>23</td>\n",
       "      <td>123.963750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>199.994062</td>\n",
       "      <td>211.654688</td>\n",
       "      <td>2</td>\n",
       "      <td>11.660625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>212.346563</td>\n",
       "      <td>212.633438</td>\n",
       "      <td>1</td>\n",
       "      <td>0.286875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>212.633438</td>\n",
       "      <td>215.215312</td>\n",
       "      <td>2</td>\n",
       "      <td>2.581875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>216.430313</td>\n",
       "      <td>263.275313</td>\n",
       "      <td>19</td>\n",
       "      <td>46.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>264.237188</td>\n",
       "      <td>269.114063</td>\n",
       "      <td>1</td>\n",
       "      <td>4.876875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>269.654063</td>\n",
       "      <td>289.600313</td>\n",
       "      <td>6</td>\n",
       "      <td>19.946250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>291.861563</td>\n",
       "      <td>303.404063</td>\n",
       "      <td>4</td>\n",
       "      <td>11.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>303.404063</td>\n",
       "      <td>349.270313</td>\n",
       "      <td>15</td>\n",
       "      <td>45.866250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>350.468438</td>\n",
       "      <td>360.812813</td>\n",
       "      <td>2</td>\n",
       "      <td>10.344375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>361.521563</td>\n",
       "      <td>414.880313</td>\n",
       "      <td>15</td>\n",
       "      <td>53.358750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>415.403438</td>\n",
       "      <td>420.027188</td>\n",
       "      <td>2</td>\n",
       "      <td>4.623750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>420.246563</td>\n",
       "      <td>459.109688</td>\n",
       "      <td>11</td>\n",
       "      <td>38.863125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>460.020938</td>\n",
       "      <td>469.909688</td>\n",
       "      <td>4</td>\n",
       "      <td>9.888750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>470.719688</td>\n",
       "      <td>495.441563</td>\n",
       "      <td>9</td>\n",
       "      <td>24.721875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>495.441563</td>\n",
       "      <td>495.492188</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>495.492188</td>\n",
       "      <td>505.094063</td>\n",
       "      <td>6</td>\n",
       "      <td>9.601875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>506.292188</td>\n",
       "      <td>526.069688</td>\n",
       "      <td>9</td>\n",
       "      <td>19.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>526.069688</td>\n",
       "      <td>558.199688</td>\n",
       "      <td>12</td>\n",
       "      <td>32.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>559.043438</td>\n",
       "      <td>564.595313</td>\n",
       "      <td>1</td>\n",
       "      <td>5.551875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>564.764063</td>\n",
       "      <td>565.641563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>566.367188</td>\n",
       "      <td>572.374688</td>\n",
       "      <td>2</td>\n",
       "      <td>6.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>572.847188</td>\n",
       "      <td>582.786563</td>\n",
       "      <td>1</td>\n",
       "      <td>9.939375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       speaker       start         end  segment_marker_count  segment_len\n",
       "0   SPEAKER_01    4.716563   22.283438                     4    17.566875\n",
       "1   SPEAKER_03   22.857187   71.052188                    10    48.195000\n",
       "2   SPEAKER_01   71.254688   74.950312                     3     3.695625\n",
       "3   SPEAKER_03   74.950312  198.914062                    23   123.963750\n",
       "4   SPEAKER_01  199.994062  211.654688                     2    11.660625\n",
       "5   SPEAKER_00  212.346563  212.633438                     1     0.286875\n",
       "6   SPEAKER_01  212.633438  215.215312                     2     2.581875\n",
       "7   SPEAKER_00  216.430313  263.275313                    19    46.845000\n",
       "8   SPEAKER_01  264.237188  269.114063                     1     4.876875\n",
       "9   SPEAKER_00  269.654063  289.600313                     6    19.946250\n",
       "10  SPEAKER_01  291.861563  303.404063                     4    11.542500\n",
       "11  SPEAKER_00  303.404063  349.270313                    15    45.866250\n",
       "12  SPEAKER_01  350.468438  360.812813                     2    10.344375\n",
       "13  SPEAKER_00  361.521563  414.880313                    15    53.358750\n",
       "14  SPEAKER_01  415.403438  420.027188                     2     4.623750\n",
       "15  SPEAKER_00  420.246563  459.109688                    11    38.863125\n",
       "16  SPEAKER_01  460.020938  469.909688                     4     9.888750\n",
       "17  SPEAKER_00  470.719688  495.441563                     9    24.721875\n",
       "18  SPEAKER_02  495.441563  495.492188                     1     0.050625\n",
       "19  SPEAKER_00  495.492188  505.094063                     6     9.601875\n",
       "20  SPEAKER_01  506.292188  526.069688                     9    19.777500\n",
       "21  SPEAKER_00  526.069688  558.199688                    12    32.130000\n",
       "22  SPEAKER_01  559.043438  564.595313                     1     5.551875\n",
       "23  SPEAKER_00  564.764063  565.641563                     1     0.877500\n",
       "24  SPEAKER_01  566.367188  572.374688                     2     6.007500\n",
       "25  SPEAKER_02  572.847188  582.786563                     1     9.939375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what to do with these tiny segments?\n",
    "(x\n",
    ".assign(start_min=lambda x: x.start/60)\n",
    ".assign(end_min=lambda x: x.end/60)\n",
    ".query('segment_len < 5')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_word_timestamps(asr_output, chunk_offset):\n",
    "    preds = asr_output.y_sequence.tolist()  # some funky formatting\n",
    "    probs_seq = torch.FloatTensor([preds])  # some funky formatting\n",
    "    beam_result, beam_scores, timesteps, out_seq_len = decoder.decode(probs_seq)\n",
    "    lens = out_seq_len[0][0]\n",
    "    timesteps = timesteps[0][0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    if len(timesteps) == 0:\n",
    "        return result\n",
    "\n",
    "    start = (timesteps[0] - TIME_PAD) * time_stride + chunk_offset\n",
    "    end = (timesteps[0] + TIME_PAD * 2) * time_stride + chunk_offset\n",
    "\n",
    "    token_prev = vocab[int(beam_result[0][0][0])]\n",
    "    word = token_prev\n",
    "\n",
    "    for n in range(1, lens):\n",
    "        token = vocab[int(beam_result[0][0][n])]\n",
    "\n",
    "        if token[0] == \"#\":\n",
    "            # merge subwords\n",
    "            word = word + token[2:]\n",
    "\n",
    "        elif token[0] == \"-\" or token_prev[0] == \"-\":\n",
    "            word = word + token\n",
    "\n",
    "        else:\n",
    "            word = word.replace(\"▁\", \"\").replace(\"_\", \"\")  # remove weird token\n",
    "\n",
    "            result_word = {\n",
    "                \"startTime\": int(start) / 1000,\n",
    "                \"endTime\": int(end) / 1000,\n",
    "                \"word\": word,\n",
    "            }\n",
    "            result.append(result_word)\n",
    "\n",
    "            start = (timesteps[n] - TIME_PAD) * time_stride + chunk_offset\n",
    "            word = token\n",
    "\n",
    "        end = (timesteps[n] + TIME_PAD * 2) * time_stride + chunk_offset\n",
    "        token_prev = token\n",
    "\n",
    "    # add last word\n",
    "    word = word.replace(\"▁\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "    result_word = {\n",
    "        \"startTime\": int(start) / 1000,\n",
    "        \"endTime\": int(end) / 1000,\n",
    "        \"word\": word,\n",
    "    }\n",
    "    result.append(result_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _naively_segment_utterances(record):\n",
    "    # apply naive splitting\n",
    "    n_chunks = int((record.end - record.start) // second_max_audio) + 1\n",
    "    chunk_len = (record.end - record.start) / n_chunks\n",
    "\n",
    "    df_temp = pd.DataFrame([record] * n_chunks).reset_index(drop=True)\n",
    "    df_temp[\"start\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len * x.name, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp[\"end\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp.loc[\n",
    "        (n_chunks - 1), \"end\"\n",
    "    ] = (\n",
    "        record.end\n",
    "    )  # adjust end time to actual time (sanity correction in case rounding cuts of audio)\n",
    "    return df_temp.assign(segment_len=lambda x: x.end - x.start)\n",
    "\n",
    "\n",
    "def _segment_utterances(audio_segment, record):\n",
    "    dBFS = audio_segment.dBFS \n",
    "    # TODO: optimise silence threshold magic number\n",
    "    silences = detect_silence(\n",
    "        audio_segment, min_silence_len=500, silence_thresh=dBFS - 20\n",
    "    )  # 0.5 break, time in ms, silence_thresh 20 DB lower than audio volume?\n",
    "\n",
    "    if len(silences) == 0:\n",
    "        # no silence detected, lower min_silence_len\n",
    "        silences = detect_silence(\n",
    "            audio_segment, min_silence_len=100, silence_thresh=dBFS - 20\n",
    "        )\n",
    "\n",
    "        if len(silences) == 0:\n",
    "            # if still no silences detected after lowering min_silence_len, split naively\n",
    "            return _naively_segment_utterances(record)\n",
    "\n",
    "    silences = [[(s[1] - s[0]), s[0] / 1000, s[1] / 1000] for s in silences]  # ms -> s\n",
    "\n",
    "    df_temp = pd.DataFrame(record).T.reset_index(drop=True)\n",
    "\n",
    "    # split on longest silence, in middle of silence so no info is lost\n",
    "    while (len(silences) > 0) & any(df_temp.segment_len > second_max_audio):\n",
    "        longest_silence = silences.pop(silences.index(max(silences)))\n",
    "        middle_silence = record.start + (\n",
    "            longest_silence[1] + (longest_silence[2] - longest_silence[1]) / 2\n",
    "        )\n",
    "\n",
    "        record_to_split = df_temp.query(\n",
    "            f\"start < {middle_silence} & end>{middle_silence} & segment_len > {second_max_audio}\"\n",
    "        )\n",
    "        df_temp = df_temp.drop(record_to_split.index)\n",
    "\n",
    "        split_utterances = pd.DataFrame(\n",
    "            [record_to_split.iloc[0], record_to_split.iloc[0]]\n",
    "        ).reset_index(drop=True)\n",
    "        split_utterances.loc[0, \"end\"] = middle_silence\n",
    "        split_utterances.loc[1, \"start\"] = middle_silence\n",
    "        df_temp = (\n",
    "            pd.concat([df_temp, split_utterances])\n",
    "            .reset_index(drop=True)\n",
    "            .assign(segment_len=lambda x: x.end - x.start)\n",
    "        )\n",
    "\n",
    "    if any(df_temp.segment_len > second_max_audio):\n",
    "        # if any segments are still too long, naively split them\n",
    "        final_df = [df_temp.query(f\"segment_len < {second_max_audio}\")]\n",
    "        records_to_split = df_temp.query(f\"segment_len > {second_max_audio}\")\n",
    "\n",
    "        for i, record in records_to_split.iterrows():\n",
    "            final_df.append(_naively_segment_utterances(record))\n",
    "        return pd.concat(final_df).reset_index(drop=True).sort_values(by=[\"start\"])\n",
    "\n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "import string\n",
    "\n",
    "_transcribe_mono('../output/radio_national_podcasts/audio/methadone-stigma.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transcribe_mono(input_file, single_speaker=False):\n",
    "    # transcribe a mono wav file\n",
    "    input_file = Path(input_file)\n",
    "    asr_logger.info(f\"Transcribing: {input_file}..\")\n",
    "\n",
    "    temp_dir = Path('../output/temp_dir')\n",
    "    shutil.rmtree(str(temp_dir)) if temp_dir.exists() else None\n",
    "    temp_dir.mkdir(parents=True)\n",
    "\n",
    "    # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # 1.0 resample, convert to wav\n",
    "    wav_path = _resample_normalize_audio(\n",
    "        input_file, str(Path(temp_dir) / f\"{Path(input_file).stem}.wav\")\n",
    "    )\n",
    "    audio_segment = AudioSegment.from_file(wav_path)\n",
    "    asr_logger.info('Successfully resampled/converted input to WAV')\n",
    "\n",
    "    # 2.0 diarize input, save diarised segments\n",
    "    diarized_segments = _diarize_mono_audio(wav_path, single_speaker)\n",
    "    paths2audio_files = []  # explicitly sequence, RE: sorted() issues\n",
    "\n",
    "    chunked_diarized_segments = []\n",
    "    for idx, record in diarized_segments.iterrows():\n",
    "        if record.segment_len > second_max_audio:\n",
    "            records = _segment_utterances(\n",
    "                audio_segment[floor(record.start * 1000) : ceil(record.end * 1000)],\n",
    "                record,\n",
    "            )\n",
    "            chunked_diarized_segments.append(records)\n",
    "        else:\n",
    "            chunked_diarized_segments.append(\n",
    "                pd.DataFrame(record).T.reset_index(drop=True)\n",
    "            )\n",
    "    chunked_diarized_segments = pd.concat(chunked_diarized_segments).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    asr_logger.info('Successfully diarised input')\n",
    "\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        # slice audio per utterance, round start/end to floor/ceil inclusively\n",
    "        segment_audio = audio_segment[\n",
    "            floor(record.start * 1000) : ceil(record.end * 1000)\n",
    "        ]\n",
    "\n",
    "        # prevent misc output from printing\n",
    "        segment_audio_res = segment_audio.export(\n",
    "            Path(temp_dir) / f\"chunk_{idx}.wav\", format=\"wav\"\n",
    "        )\n",
    "        # collect segment audio path\n",
    "        paths2audio_files.append(str(Path(temp_dir) / f\"chunk_{idx}.wav\"))\n",
    "    asr_logger.info('Successfully chunked and saved diarised temp chunks')\n",
    "\n",
    "    # 3.0 batch transcribe, retrieve transcripts, alignments and logprobs for each utterance\n",
    "    outputs = asr_model.transcribe(\n",
    "        paths2audio_files=paths2audio_files,\n",
    "        batch_size=batch_size,\n",
    "        return_hypotheses=True,\n",
    "    )\n",
    "    asr_logger.info('Successfully processed chunks with ASR model')\n",
    "\n",
    "    # 4.0 retrieve/format timestamps\n",
    "    time_formatted_words_all = []\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        time_formatted_words = _format_word_timestamps(outputs[idx], record.start)\n",
    "\n",
    "        # 5.0 apply punctuation to each output\n",
    "        punctuated_sequence = punct_model.add_punctuation_capitalization(\n",
    "            [\" \".join(e[\"word\"] for e in time_formatted_words)]\n",
    "        )[0]\n",
    "\n",
    "        if len(punctuated_sequence.split(\" \")) == len(time_formatted_words):\n",
    "            # easy case, where punctuated output len matches input len; assign directly\n",
    "            punctuated_sequence_joined = (\n",
    "                pd.DataFrame(time_formatted_words)\n",
    "                .assign(word=punctuated_sequence.split(\" \"))\n",
    "                .assign(speakerTag=record.speaker)\n",
    "                .to_dict(orient=\"records\")\n",
    "            )\n",
    "            time_formatted_words_all.append(punctuated_sequence_joined)\n",
    "        else:\n",
    "            # otherwise.. pad the difference? changes should be limited to immediately proceeding fullstops, commas, question marks\n",
    "            # https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html\n",
    "            print(\"Punctuated outputs not the same length as input\")\n",
    "\n",
    "    return time_formatted_words_all\n",
    "\n",
    "\n",
    "def _transcribe_channel_seperated_audio(input_file):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # 1.0 split left/right channels\n",
    "        left_channel, right_channel = _split_stereo_audio(input_file, temp_dir)\n",
    "\n",
    "        # 2.0 process as seperate monos\n",
    "        left_res = _transcribe_mono(left_channel, single_speaker=True)\n",
    "        right_res = _transcribe_mono(right_channel, single_speaker=True)\n",
    "\n",
    "    # 3.0 merge outputs\n",
    "    return _gcp_format_channel_seperated_transcript_objects(left_res, right_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
