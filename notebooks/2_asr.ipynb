{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-08-19 07:27:51 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-08-19 07:27:53 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-08-19 07:27:53 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "from pyannote.audio import Pipeline\n",
    "import shutil\n",
    "\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)\n",
    "asr_logger = logging.getLogger(\"asr\")\n",
    "asr_logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dia_model_name = \"pyannote/speaker-diarization@2022.07\"\n",
    "asr_model_name = \"stt_en_conformer_ctc_small\"  #'QuartzNet15x5Base-En'\n",
    "# asr_model_name = \"QuartzNet15x5Base-En\"\n",
    "punct_model_name = \"punctuation_en_bert\"\n",
    "\n",
    "dia_model = Pipeline.from_pretrained(dia_model_name)\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=asr_model_name)\n",
    "punct_model = punct_model = PunctuationCapitalizationModel.from_pretrained(punct_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "pause_threshold = 1  # RE: collapsing diarised segments\n",
    "batch_size = 4\n",
    "offset = -0.18  # calibration offset for timestamps: 180 ms\n",
    "\n",
    "# load models\n",
    "vocab = asr_model.decoder.vocabulary\n",
    "vocab.append(\"_\")\n",
    "decoder = CTCBeamDecoder(\n",
    "    vocab,\n",
    "    beam_width=1,\n",
    "    blank_id=vocab.index(\"_\"),\n",
    "    log_probs_input=True,\n",
    ")\n",
    "time_stride = 1 / asr_model.cfg.preprocessor.window_size  # duration of model timesteps\n",
    "\n",
    "# from_disk(model_path)\n",
    "time_pad = 1\n",
    "# huge possible max audio if model is Quartznet; maximise where possible to limit memory overflow errors\n",
    "second_max_audio = 120 if asr_model_name == \"QuartzNet15x5Base-En\" else 8 # 4 base amount? > add rough memory calculation\n",
    "round_value = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample_normalize_audio(in_file, out_file, sample_rate=16000):\n",
    "    # upsample/normalize audio to 16khz WAV\n",
    "    # via https://github.com/NVIDIA/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb\n",
    "    if not os.path.exists(in_file):\n",
    "        raise ValueError(f\"{in_file} not found\")\n",
    "    if out_file is None:\n",
    "        out_file = in_file.replace(os.path.splitext(in_file)[-1], f\"_{sample_rate}.wav\")\n",
    "\n",
    "    os.system(\n",
    "        f\"ffmpeg -i {in_file} -acodec pcm_s16le -ac 1 -af aresample=resampler=soxr -ar {sample_rate} {out_file} -y\"\n",
    "    )\n",
    "    return out_file\n",
    "\n",
    "def _diarize_mono_audio(in_file, audio_segment):\n",
    "    diarization_raw = dia_model(str(in_file))\n",
    "    diarized_segments = (\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n",
    "                for turn, _, speaker in diarization_raw.itertracks(yield_label=True)\n",
    "            ]\n",
    "        )\n",
    "        # shift speaker attribution > mark/collapse consecutive speaker segments\n",
    "        .assign(segment_marker=lambda x: x.speaker.shift(1)).assign(\n",
    "            segment_marker=lambda x: x.segment_marker != x.speaker\n",
    "        )\n",
    "        .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker))\n",
    "    )\n",
    "\n",
    "    diarized_segments = (\n",
    "        diarized_segments\n",
    "        # groupby/aggregate shifted, collapse consecutive speaker sequences\n",
    "        .groupby(\"segment_marker\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"speaker\": \"first\",\n",
    "                \"start\": \"first\",\n",
    "                \"end\": \"last\",\n",
    "                \"segment_marker\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .rename(\n",
    "            mapper={\"segment_marker\": \"segment_marker_count\"},\n",
    "            axis=\"columns\",\n",
    "            inplace=False,\n",
    "        )\n",
    "        .assign(segment_len=lambda x: x.end - x.start)\n",
    "        # TODO: finesse a merging strategy\n",
    "        .query('segment_len >= @pause_threshold')\n",
    "        .reset_index(drop=True)\n",
    "        .assign(audio_segment=lambda x: x.apply(lambda y: _assign_child_segment(y, audio_segment), axis=1))\n",
    "    )\n",
    "\n",
    "    return diarized_segments\n",
    "\n",
    "def _assign_child_segment(record, parent_audio_segment):\n",
    "    return parent_audio_segment[record.start*1000:record.end*1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = _diarize_mono_audio('../output/temp_dir/methadone-stigma.wav', audio_segment)\n",
    "\n",
    "# # what to do with these tiny segments?\n",
    "# (x\n",
    "# .assign(start_min=lambda x: x.start/60)\n",
    "# .assign(end_min=lambda x: x.end/60)\n",
    "# .query('segment_len < 5')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_word_timestamps(asr_output, chunk_offset):\n",
    "    preds = asr_output.y_sequence.tolist()  # some funky formatting\n",
    "    probs_seq = torch.FloatTensor([preds])  # some funky formatting\n",
    "    beam_result, beam_scores, timesteps, out_seq_len = decoder.decode(probs_seq)\n",
    "    lens = out_seq_len[0][0]\n",
    "    timesteps = timesteps[0][0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    if asr_output.text == '':\n",
    "        # silence\n",
    "        return result\n",
    "\n",
    "    if len(timesteps) == 0:\n",
    "        # probably impossible?\n",
    "        return result\n",
    "\n",
    "    start = (timesteps[0] - time_pad) * time_stride + chunk_offset\n",
    "    end = (timesteps[0] + time_pad * 2) * time_stride + chunk_offset\n",
    "\n",
    "    token_prev = vocab[int(beam_result[0][0][0])]\n",
    "    word = token_prev\n",
    "\n",
    "    for n in range(1, lens):\n",
    "        token = vocab[int(beam_result[0][0][n])]\n",
    "\n",
    "        if token[0] == \"#\":\n",
    "            # merge subwords\n",
    "            word = word + token[2:]\n",
    "\n",
    "        elif token[0] == \"-\" or token_prev[0] == \"-\":\n",
    "            word = word + token\n",
    "\n",
    "        else:\n",
    "            word = word.replace(\"▁\", \"\").replace(\"_\", \"\")  # remove weird token\n",
    "\n",
    "            result_word = {\n",
    "                \"startTime\": int(start) / 1000,\n",
    "                \"endTime\": int(end) / 1000,\n",
    "                \"word\": word,\n",
    "            }\n",
    "            result.append(result_word)\n",
    "\n",
    "            start = (timesteps[n] - time_pad) * time_stride + chunk_offset\n",
    "            word = token\n",
    "\n",
    "        end = (timesteps[n] + time_pad * 2) * time_stride + chunk_offset\n",
    "        token_prev = token\n",
    "\n",
    "    # add last word\n",
    "    word = word.replace(\"▁\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "    result_word = {\n",
    "        \"startTime\": int(start) / 1000,\n",
    "        \"endTime\": int(end) / 1000,\n",
    "        \"word\": word,\n",
    "    }\n",
    "    result.append(result_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transcription' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blog-os-asr/notebooks/2_asr.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=72'>73</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df_temp\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=76'>77</a>\u001b[0m audio_segment \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39mfrom_file(\u001b[39m'\u001b[39m\u001b[39m../output/temp_dir/methadone-stigma.wav\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=77'>78</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, record \u001b[39min\u001b[39;00m transcription\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=78'>79</a>\u001b[0m     _segment_utterances(\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=79'>80</a>\u001b[0m                 audio_segment[floor(record\u001b[39m.\u001b[39mstart \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m) : ceil(record\u001b[39m.\u001b[39mend \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)],\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=80'>81</a>\u001b[0m                 record,\n\u001b[1;32m     <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/2_asr.ipynb#ch0000006vscode-remote?line=81'>82</a>\u001b[0m             )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transcription' is not defined"
     ]
    }
   ],
   "source": [
    "# def _naively_segment_utterances(record):\n",
    "#     # apply naive splitting\n",
    "#     n_chunks = int((record.end - record.start) // second_max_audio) + 1\n",
    "#     chunk_len = (record.end - record.start) / n_chunks\n",
    "\n",
    "#     df_temp = pd.DataFrame([record] * n_chunks).reset_index(drop=True)\n",
    "#     df_temp[\"start\"] = df_temp.apply(\n",
    "#         lambda x: x.start + chunk_len * x.name, axis=1\n",
    "#     )  # increase start time\n",
    "#     df_temp[\"end\"] = df_temp.apply(\n",
    "#         lambda x: x.start + chunk_len, axis=1\n",
    "#     )  # increase start time\n",
    "#     df_temp.loc[\n",
    "#         (n_chunks - 1), \"end\"\n",
    "#     ] = (\n",
    "#         record.end\n",
    "#     )  # adjust end time to actual time (sanity correction in case rounding cuts of audio)\n",
    "#     return df_temp.assign(segment_len=lambda x: x.end - x.start)\n",
    "\n",
    "\n",
    "# def _segment_utterances(audio_segment, record):\n",
    "#     dBFS = audio_segment.dBFS \n",
    "#     # TODO: optimise silence threshold magic number\n",
    "#     silences = detect_silence(\n",
    "#         audio_segment, min_silence_len=500, silence_thresh=dBFS - 20\n",
    "#     )  # 0.5 break, time in ms, silence_thresh 20 DB lower than audio volume?\n",
    "\n",
    "#     if len(silences) == 0:\n",
    "#         # no silence detected, lower min_silence_len\n",
    "#         silences = detect_silence(\n",
    "#             audio_segment, min_silence_len=100, silence_thresh=dBFS - 20\n",
    "#         )\n",
    "\n",
    "#         if len(silences) == 0:\n",
    "#             # if still no silences detected after lowering min_silence_len, split naively\n",
    "#             return _naively_segment_utterances(record)\n",
    "\n",
    "#     silences = [[(s[1] - s[0]), s[0] / 1000, s[1] / 1000] for s in silences]  # ms -> s\n",
    "#     df_temp = pd.DataFrame(record).T.reset_index(drop=True)\n",
    "\n",
    "#     # split on longest silence, in middle of silence so no info is lost\n",
    "#     while (len(silences) > 0) & any(df_temp.segment_len > second_max_audio):\n",
    "#         longest_silence = silences.pop(silences.index(max(silences)))\n",
    "#         middle_silence = record.start + (\n",
    "#             longest_silence[1] + (longest_silence[2] - longest_silence[1]) / 2\n",
    "#         )\n",
    "\n",
    "#         record_to_split = df_temp.query(\n",
    "#             f\"start < {middle_silence} & end>{middle_silence} & segment_len > {second_max_audio}\"\n",
    "#         )\n",
    "#         df_temp = df_temp.drop(record_to_split.index)\n",
    "\n",
    "#         split_utterances = pd.DataFrame(\n",
    "#             [record_to_split.iloc[0], record_to_split.iloc[0]]\n",
    "#         ).reset_index(drop=True)\n",
    "#         split_utterances.loc[0, \"end\"] = middle_silence\n",
    "#         split_utterances.loc[1, \"start\"] = middle_silence\n",
    "#         df_temp = (\n",
    "#             pd.concat([df_temp, split_utterances])\n",
    "#             .reset_index(drop=True)\n",
    "#             .assign(segment_len=lambda x: x.end - x.start)\n",
    "#         )\n",
    "\n",
    "#     if any(df_temp.segment_len > second_max_audio):\n",
    "#         # if any segments are still too long, naively split them\n",
    "#         final_df = [df_temp.query(f\"segment_len < {second_max_audio}\")]\n",
    "#         records_to_split = df_temp.query(f\"segment_len > {second_max_audio}\")\n",
    "\n",
    "#         for i, record in records_to_split.iterrows():\n",
    "#             final_df.append(_naively_segment_utterances(record))\n",
    "#         return pd.concat(final_df).reset_index(drop=True).sort_values(by=[\"start\"])\n",
    "\n",
    "#     return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import utils\n",
    "\n",
    "def psuedo_optimise_silence_split(audio_segment):\n",
    "    # note, silence splitting has effect of reducing broader segment > small amounts of drift\n",
    "    dbfs_diff = 10\n",
    "    max_dbfs_diff = 40\n",
    "    min_silence_len = 500\n",
    "    audio_segments = pydub.silence.split_on_silence(audio_segment, min_silence_len=min_silence_len, silence_thresh=dBFS - dbfs_diff)\n",
    "    while pd.Series([e.duration_seconds for e in audio_segments]).median() >= second_max_audio and dbfs_diff <= max_dbfs_diff:\n",
    "        print('trying again')\n",
    "        dbfs_diff += 10\n",
    "        audio_segments = pydub.silence.split_on_silence(audio_segment, min_silence_len=min_silence_len, silence_thresh=dBFS - dbfs_diff)\n",
    "        \n",
    "    return audio_segments\n",
    "\n",
    "def _segment_utterances(record):\n",
    "    if record.segment_len > second_max_audio:\n",
    "        silence_splits = psuedo_optimise_silence_split(record.audio_segment)\n",
    "        all_splits = []\n",
    "        for split in silence_splits:\n",
    "            if split.duration_seconds > second_max_audio:\n",
    "                print(split.duration_seconds)\n",
    "                all_splits.extend(utils.make_chunks(split, second_max_audio*1000))\n",
    "            else:\n",
    "                all_splits.append(split)\n",
    "\n",
    "        start_times = []\n",
    "        start_time = record.start\n",
    "        for e in all_splits:\n",
    "            start_times.append(start_time)\n",
    "            start_time += e.duration_seconds\n",
    "\n",
    "        segments= (pd.DataFrame([{'audio_segment':e, 'speaker': record.speaker, 'segment_len': e.duration_seconds} for e in all_splits])\n",
    "        .assign(start=start_times)\n",
    "        .assign(end=lambda x: x.start + x.segment_len)\n",
    "        )\n",
    "        print(record.end, segments.end.max())\n",
    "        return segments\n",
    "    else:\n",
    "        return record.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub\n",
    "audio_segment = AudioSegment.from_file('../output/temp_dir/methadone-stigma.wav')\n",
    "\n",
    "for idx, record in transcription.query('speaker == \"SPEAKER_04\"').iterrows():\n",
    "    dBFS = audio_segment.dBFS \n",
    "    segment_slice = audio_segment[floor(record.start*1000) : ceil(record.end*1000)]\n",
    "    # audio_segments = pydub.silence.split_on_silence(segment_slice, min_silence_len=150, silence_thresh=-10)\n",
    "    audio_segments= psuedo_optimise_silence_split(segment_slice)\n",
    "    len(audio_segments)\n",
    "    [len(e) / 1000 for e in audio_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "def _transcribe_mono(input_file, single_speaker=False):\n",
    "    # transcribe a mono wav file\n",
    "    input_file = Path(input_file)\n",
    "    asr_logger.info(f\"Transcribing: {input_file}..\")\n",
    "\n",
    "    temp_dir = Path('../output/temp_dir')\n",
    "    shutil.rmtree(str(temp_dir)) if temp_dir.exists() else None\n",
    "    temp_dir.mkdir(parents=True)\n",
    "\n",
    "    # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # 1.0 resample, convert to wav\n",
    "    wav_path = _resample_normalize_audio(\n",
    "        input_file, str(Path(temp_dir) / f\"{Path(input_file).stem}.wav\")\n",
    "    )\n",
    "    audio_segment = AudioSegment.from_file(wav_path)\n",
    "    asr_logger.info('Successfully resampled/converted input to WAV')\n",
    "\n",
    "    # 2.0 diarize input, save diarised segments\n",
    "    diarized_segments = _diarize_mono_audio(wav_path, audio_segment)\n",
    "    asr_logger.info('Successfully diarized input')\n",
    "    paths2audio_files = []  # explicitly sequence, RE: sorted() issues\n",
    "\n",
    "    chunked_diarized_segments = transcription.apply(lambda x: _segment_utterances(x), axis=1)\n",
    "    chunked_diarized_segments = pd.concat(chunked_diarized_segments.tolist()).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    asr_logger.info('Successfully chunked diarized input')\n",
    "    return chunked_diarized_segments\n",
    "\n",
    "    # chunked_diarized_segments = []\n",
    "    # return diarized_segments\n",
    "    # for idx, record in diarized_segments.iterrows():\n",
    "    #     if record.segment_len > second_max_audio:\n",
    "    #         records = _segment_utterances(\n",
    "    #             audio_segment[floor(record.start * 1000) : ceil(record.end * 1000)],\n",
    "    #             record,\n",
    "    #         )\n",
    "    #         chunked_diarized_segments.append(records)\n",
    "    #     else:\n",
    "    #         chunked_diarized_segments.append(\n",
    "    #             pd.DataFrame(record).T.reset_index(drop=True)\n",
    "    #         )\n",
    "    # chunked_diarized_segments = pd.concat(chunked_diarized_segments).reset_index(\n",
    "    #     drop=True\n",
    "    # )\n",
    "    \n",
    "\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        # TODO: round to transient/amplitude spike instead\n",
    "        segment_audio_res = record.segment.export(\n",
    "            Path(temp_dir) / f\"chunk_{idx}.wav\", format=\"wav\"\n",
    "        )\n",
    "        paths2audio_files.append(str(Path(temp_dir) / f\"chunk_{idx}.wav\"))\n",
    "    asr_logger.info('Successfully saved diarized segments')\n",
    "\n",
    "    # 3.0 batch transcribe, retrieve transcripts, alignments and logprobs for each utterance\n",
    "    outputs = asr_model.transcribe(\n",
    "        paths2audio_files=paths2audio_files,\n",
    "        batch_size=batch_size,\n",
    "        return_hypotheses=True,\n",
    "    )\n",
    "    asr_logger.info('Successfully processed segments with ASR model')\n",
    "\n",
    "    # 4.0 retrieve/format timestamps\n",
    "    time_formatted_words_all = []\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        time_formatted_words = _format_word_timestamps(outputs[idx], record.start)\n",
    "\n",
    "        # 5.0 apply punctuation to each output\n",
    "        punctuated_sequence = punct_model.add_punctuation_capitalization(\n",
    "            [\" \".join(e[\"word\"] for e in time_formatted_words)]\n",
    "        )[0]\n",
    "\n",
    "        if len(punctuated_sequence.split(\" \")) == len(time_formatted_words):\n",
    "            # easy case, where punctuated output len matches input len; assign directly\n",
    "            punctuated_sequence_joined = (\n",
    "                pd.DataFrame(time_formatted_words)\n",
    "                .assign(word=punctuated_sequence.split(\" \"))\n",
    "                .assign(speakerTag=record.speaker)\n",
    "                .to_dict(orient=\"records\")\n",
    "            )\n",
    "            time_formatted_words_all.append(punctuated_sequence_joined)\n",
    "        else:\n",
    "            # otherwise.. pad the difference? changes should be limited to immediately proceeding fullstops, commas, question marks\n",
    "            # https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html\n",
    "            print(\"Punctuated outputs not the same length as input\")\n",
    "\n",
    "    return time_formatted_words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mp3, from '../output/radio_national_podcasts/audio/methadone-stigma.mp3':\n",
      "  Duration: 00:09:44.98, start: 0.023021, bitrate: 128 kb/s\n",
      "    Stream #0:0: Audio: mp3, 48000 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : LAME3.98r\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '../output/temp_dir/methadone-stigma.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.29.100\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 pcm_s16le\n",
      "size=   18279kB time=00:09:44.93 bitrate= 256.0kbits/s speed= 496x    \n",
      "video:0kB audio:18279kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000417%\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "import string\n",
    "\n",
    "transcription = _transcribe_mono('../output/radio_national_podcasts/audio/methadone-stigma.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(pd.DataFrame(transcription[5]).word.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
