{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "from pyannote.audio import Pipeline\n",
    "import shutil\n",
    "\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)\n",
    "asr_logger = logging.getLogger(\"asr\")\n",
    "asr_logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_model_name = \"pyannote/speaker-diarization@2022.07\"\n",
    "asr_model_name = \"stt_en_conformer_ctc_small\"  #'QuartzNet15x5Base-En'\n",
    "# asr_model_name = \"QuartzNet15x5Base-En\"\n",
    "punct_model_name = \"punctuation_en_bert\"\n",
    "\n",
    "dia_model = Pipeline.from_pretrained(dia_model_name)\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=asr_model_name)\n",
    "punct_model = punct_model = PunctuationCapitalizationModel.from_pretrained(punct_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "pause_threshold = 1  # RE: collapsing diarised segments\n",
    "batch_size = 4\n",
    "offset = -0.18  # calibration offset for timestamps: 180 ms\n",
    "\n",
    "# load models\n",
    "vocab = asr_model.decoder.vocabulary\n",
    "vocab.append(\"_\")\n",
    "decoder = CTCBeamDecoder(\n",
    "    vocab,\n",
    "    beam_width=1,\n",
    "    blank_id=vocab.index(\"_\"),\n",
    "    log_probs_input=True,\n",
    ")\n",
    "time_stride = 1 / asr_model.cfg.preprocessor.window_size  # duration of model timesteps\n",
    "\n",
    "# from_disk(model_path)\n",
    "time_pad = 1\n",
    "# huge possible max audio if model is Quartznet; maximise where possible to limit memory overflow errors\n",
    "second_max_audio = 120 if asr_model_name == \"QuartzNet15x5Base-En\" else 8 # 4 base amount? > add rough memory calculation\n",
    "round_value = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample_normalize_audio(in_file, out_file, sample_rate=16000):\n",
    "    # upsample/normalize audio to 16khz WAV\n",
    "    # via https://github.com/NVIDIA/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb\n",
    "    if not os.path.exists(in_file):\n",
    "        raise ValueError(f\"{in_file} not found\")\n",
    "    if out_file is None:\n",
    "        out_file = in_file.replace(os.path.splitext(in_file)[-1], f\"_{sample_rate}.wav\")\n",
    "\n",
    "    os.system(\n",
    "        f\"ffmpeg -i {in_file} -acodec pcm_s16le -ac 1 -af aresample=resampler=soxr -ar {sample_rate} {out_file} -y\"\n",
    "    )\n",
    "    return out_file\n",
    "\n",
    "def _diarize_mono_audio(in_file):\n",
    "    diarization_raw = dia_model(str(in_file))\n",
    "    diarized_segments = (\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n",
    "                for turn, _, speaker in diarization_raw.itertracks(yield_label=True)\n",
    "            ]\n",
    "        )\n",
    "        # shift speaker attribution > mark/collapse consecutive speaker segments\n",
    "        .assign(segment_marker=lambda x: x.speaker.shift(1)).assign(\n",
    "            segment_marker=lambda x: x.segment_marker != x.speaker\n",
    "        )\n",
    "        .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker))\n",
    "    )\n",
    "\n",
    "    diarized_segments = (\n",
    "        diarized_segments\n",
    "        # groupby/aggregate shifted, collapse consecutive speaker sequences\n",
    "        .groupby(\"segment_marker\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"speaker\": \"first\",\n",
    "                \"start\": \"first\",\n",
    "                \"end\": \"last\",\n",
    "                \"segment_marker\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .rename(\n",
    "            mapper={\"segment_marker\": \"segment_marker_count\"},\n",
    "            axis=\"columns\",\n",
    "            inplace=False,\n",
    "        )\n",
    "        .assign(segment_len=lambda x: x.end - x.start)\n",
    "        # reconcile very short segments with pre/proceeding segment? merging strategy?\n",
    "        #             .query('segment_len >= 1')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return diarized_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = _diarize_mono_audio('../output/temp_dir/methadone-stigma.wav')\n",
    "\n",
    "# what to do with these tiny segments?\n",
    "(x\n",
    ".assign(start_min=lambda x: x.start/60)\n",
    ".assign(end_min=lambda x: x.end/60)\n",
    ".query('segment_len < 5')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_word_timestamps(asr_output, chunk_offset):\n",
    "    preds = asr_output.y_sequence.tolist()  # some funky formatting\n",
    "    probs_seq = torch.FloatTensor([preds])  # some funky formatting\n",
    "    beam_result, beam_scores, timesteps, out_seq_len = decoder.decode(probs_seq)\n",
    "    lens = out_seq_len[0][0]\n",
    "    timesteps = timesteps[0][0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    if asr_output.text == '':\n",
    "        # silence\n",
    "        return result\n",
    "\n",
    "    if len(timesteps) == 0:\n",
    "        # probably impossible?\n",
    "        return result\n",
    "\n",
    "    start = (timesteps[0] - time_pad) * time_stride + chunk_offset\n",
    "    end = (timesteps[0] + time_pad * 2) * time_stride + chunk_offset\n",
    "\n",
    "    token_prev = vocab[int(beam_result[0][0][0])]\n",
    "    word = token_prev\n",
    "\n",
    "    for n in range(1, lens):\n",
    "        token = vocab[int(beam_result[0][0][n])]\n",
    "\n",
    "        if token[0] == \"#\":\n",
    "            # merge subwords\n",
    "            word = word + token[2:]\n",
    "\n",
    "        elif token[0] == \"-\" or token_prev[0] == \"-\":\n",
    "            word = word + token\n",
    "\n",
    "        else:\n",
    "            word = word.replace(\"▁\", \"\").replace(\"_\", \"\")  # remove weird token\n",
    "\n",
    "            result_word = {\n",
    "                \"startTime\": int(start) / 1000,\n",
    "                \"endTime\": int(end) / 1000,\n",
    "                \"word\": word,\n",
    "            }\n",
    "            result.append(result_word)\n",
    "\n",
    "            start = (timesteps[n] - time_pad) * time_stride + chunk_offset\n",
    "            word = token\n",
    "\n",
    "        end = (timesteps[n] + time_pad * 2) * time_stride + chunk_offset\n",
    "        token_prev = token\n",
    "\n",
    "    # add last word\n",
    "    word = word.replace(\"▁\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "    result_word = {\n",
    "        \"startTime\": int(start) / 1000,\n",
    "        \"endTime\": int(end) / 1000,\n",
    "        \"word\": word,\n",
    "    }\n",
    "    result.append(result_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _naively_segment_utterances(record):\n",
    "    # apply naive splitting\n",
    "    n_chunks = int((record.end - record.start) // second_max_audio) + 1\n",
    "    chunk_len = (record.end - record.start) / n_chunks\n",
    "\n",
    "    df_temp = pd.DataFrame([record] * n_chunks).reset_index(drop=True)\n",
    "    df_temp[\"start\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len * x.name, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp[\"end\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp.loc[\n",
    "        (n_chunks - 1), \"end\"\n",
    "    ] = (\n",
    "        record.end\n",
    "    )  # adjust end time to actual time (sanity correction in case rounding cuts of audio)\n",
    "    return df_temp.assign(segment_len=lambda x: x.end - x.start)\n",
    "\n",
    "\n",
    "def _segment_utterances(audio_segment, record):\n",
    "    dBFS = audio_segment.dBFS \n",
    "    # TODO: optimise silence threshold magic number\n",
    "    silences = detect_silence(\n",
    "        audio_segment, min_silence_len=500, silence_thresh=dBFS - 20\n",
    "    )  # 0.5 break, time in ms, silence_thresh 20 DB lower than audio volume?\n",
    "\n",
    "    if len(silences) == 0:\n",
    "        # no silence detected, lower min_silence_len\n",
    "        silences = detect_silence(\n",
    "            audio_segment, min_silence_len=100, silence_thresh=dBFS - 20\n",
    "        )\n",
    "\n",
    "        if len(silences) == 0:\n",
    "            # if still no silences detected after lowering min_silence_len, split naively\n",
    "            return _naively_segment_utterances(record)\n",
    "\n",
    "    silences = [[(s[1] - s[0]), s[0] / 1000, s[1] / 1000] for s in silences]  # ms -> s\n",
    "    df_temp = pd.DataFrame(record).T.reset_index(drop=True)\n",
    "\n",
    "    # split on longest silence, in middle of silence so no info is lost\n",
    "    while (len(silences) > 0) & any(df_temp.segment_len > second_max_audio):\n",
    "        longest_silence = silences.pop(silences.index(max(silences)))\n",
    "        middle_silence = record.start + (\n",
    "            longest_silence[1] + (longest_silence[2] - longest_silence[1]) / 2\n",
    "        )\n",
    "\n",
    "        record_to_split = df_temp.query(\n",
    "            f\"start < {middle_silence} & end>{middle_silence} & segment_len > {second_max_audio}\"\n",
    "        )\n",
    "        df_temp = df_temp.drop(record_to_split.index)\n",
    "\n",
    "        split_utterances = pd.DataFrame(\n",
    "            [record_to_split.iloc[0], record_to_split.iloc[0]]\n",
    "        ).reset_index(drop=True)\n",
    "        split_utterances.loc[0, \"end\"] = middle_silence\n",
    "        split_utterances.loc[1, \"start\"] = middle_silence\n",
    "        df_temp = (\n",
    "            pd.concat([df_temp, split_utterances])\n",
    "            .reset_index(drop=True)\n",
    "            .assign(segment_len=lambda x: x.end - x.start)\n",
    "        )\n",
    "\n",
    "    if any(df_temp.segment_len > second_max_audio):\n",
    "        # if any segments are still too long, naively split them\n",
    "        final_df = [df_temp.query(f\"segment_len < {second_max_audio}\")]\n",
    "        records_to_split = df_temp.query(f\"segment_len > {second_max_audio}\")\n",
    "\n",
    "        for i, record in records_to_split.iterrows():\n",
    "            final_df.append(_naively_segment_utterances(record))\n",
    "        return pd.concat(final_df).reset_index(drop=True).sort_values(by=[\"start\"])\n",
    "\n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "def _transcribe_mono(input_file, single_speaker=False):\n",
    "    # transcribe a mono wav file\n",
    "    input_file = Path(input_file)\n",
    "    asr_logger.info(f\"Transcribing: {input_file}..\")\n",
    "\n",
    "    temp_dir = Path('../output/temp_dir')\n",
    "    shutil.rmtree(str(temp_dir)) if temp_dir.exists() else None\n",
    "    temp_dir.mkdir(parents=True)\n",
    "\n",
    "    # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # 1.0 resample, convert to wav\n",
    "    wav_path = _resample_normalize_audio(\n",
    "        input_file, str(Path(temp_dir) / f\"{Path(input_file).stem}.wav\")\n",
    "    )\n",
    "    audio_segment = AudioSegment.from_file(wav_path)\n",
    "    asr_logger.info('Successfully resampled/converted input to WAV')\n",
    "\n",
    "    # 2.0 diarize input, save diarised segments\n",
    "    diarized_segments = _diarize_mono_audio(wav_path)\n",
    "    asr_logger.info('Successfully diarized input')\n",
    "    paths2audio_files = []  # explicitly sequence, RE: sorted() issues\n",
    "\n",
    "    chunked_diarized_segments = []\n",
    "    for idx, record in diarized_segments.iterrows():\n",
    "        if record.segment_len > second_max_audio:\n",
    "            records = _segment_utterances(\n",
    "                audio_segment[floor(record.start * 1000) : ceil(record.end * 1000)],\n",
    "                record,\n",
    "            )\n",
    "            chunked_diarized_segments.append(records)\n",
    "        else:\n",
    "            chunked_diarized_segments.append(\n",
    "                pd.DataFrame(record).T.reset_index(drop=True)\n",
    "            )\n",
    "    chunked_diarized_segments = pd.concat(chunked_diarized_segments).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    asr_logger.info('Successfully chunked diarized input')\n",
    "\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        # slice audio per utterance, round start/end to floor/ceil inclusively\n",
    "        segment_audio = audio_segment[\n",
    "            floor(record.start * 1000) : ceil(record.end * 1000)\n",
    "        ]\n",
    "        # TODO: round to transient/amplitude spike instead\n",
    "        segment_audio_res = segment_audio.export(\n",
    "            Path(temp_dir) / f\"chunk_{idx}.wav\", format=\"wav\"\n",
    "        )\n",
    "        paths2audio_files.append(str(Path(temp_dir) / f\"chunk_{idx}.wav\"))\n",
    "    asr_logger.info('Successfully saved diarized segments')\n",
    "\n",
    "    # 3.0 batch transcribe, retrieve transcripts, alignments and logprobs for each utterance\n",
    "    outputs = asr_model.transcribe(\n",
    "        paths2audio_files=paths2audio_files,\n",
    "        batch_size=batch_size,\n",
    "        return_hypotheses=True,\n",
    "    )\n",
    "    asr_logger.info('Successfully processed segments with ASR model')\n",
    "\n",
    "    # 4.0 retrieve/format timestamps\n",
    "    time_formatted_words_all = []\n",
    "    for idx, record in chunked_diarized_segments.iterrows():\n",
    "        time_formatted_words = _format_word_timestamps(outputs[idx], record.start)\n",
    "\n",
    "        # 5.0 apply punctuation to each output\n",
    "        punctuated_sequence = punct_model.add_punctuation_capitalization(\n",
    "            [\" \".join(e[\"word\"] for e in time_formatted_words)]\n",
    "        )[0]\n",
    "\n",
    "        if len(punctuated_sequence.split(\" \")) == len(time_formatted_words):\n",
    "            # easy case, where punctuated output len matches input len; assign directly\n",
    "            punctuated_sequence_joined = (\n",
    "                pd.DataFrame(time_formatted_words)\n",
    "                .assign(word=punctuated_sequence.split(\" \"))\n",
    "                .assign(speakerTag=record.speaker)\n",
    "                .to_dict(orient=\"records\")\n",
    "            )\n",
    "            time_formatted_words_all.append(punctuated_sequence_joined)\n",
    "        else:\n",
    "            # otherwise.. pad the difference? changes should be limited to immediately proceeding fullstops, commas, question marks\n",
    "            # https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html\n",
    "            print(\"Punctuated outputs not the same length as input\")\n",
    "\n",
    "    return time_formatted_words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "import string\n",
    "\n",
    "transcription = _transcribe_mono('../output/radio_national_podcasts/audio/methadone-stigma.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(pd.DataFrame(transcription[5]).word.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
