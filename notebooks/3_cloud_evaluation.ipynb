{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP3 Conversion\n",
    "- Mainly for GCP input constraints, but also for consistency with os implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_normalise_audio(in_file, out_file, sample_rate=16000):\n",
    "    if not os.path.exists(in_file):\n",
    "        raise ValueError(f\"{in_file} not found\")\n",
    "    if out_file is None:\n",
    "        out_file = in_file.replace(os.path.splitext(in_file)[-1], f\"_{sample_rate}.wav\")\n",
    "\n",
    "    os.system(\n",
    "        f\"ffmpeg -i {in_file} -acodec pcm_s16le -ac 1 -af aresample=resampler=soxr -ar {sample_rate} {out_file} -y\"\n",
    "    )\n",
    "    return out_file\n",
    "\n",
    "\n",
    "transcript_manifest = (\n",
    "    pd.read_csv(\"../output/radio_national_podcasts/manifest.csv\")\n",
    "    .assign(\n",
    "        audio_path=lambda x: x.audio_path.apply(\n",
    "            lambda y: str(\n",
    "                Path(\"../output/radio_national_podcasts/audio/mp3\") / Path(y).name\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        transcript_path=lambda x: x.transcript_path.apply(\n",
    "            lambda y: str(\n",
    "                Path(\"../output/radio_national_podcasts/transcripts/ground_truth\")\n",
    "                / Path(y).name\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "output_dir = Path(\"../output/radio_national_podcasts/audio/wav\")\n",
    "shutil.rmtree(str(output_dir)) if output_dir.exists() else None\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for idx, record in transcript_manifest.iterrows():\n",
    "    input_path = Path(record.audio_path)\n",
    "    output_path = input_path.parents[1] / f\"wav/{input_path.stem}.wav\"\n",
    "    resample_normalise_audio(str(input_path), str(output_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from asr import transcribe_mono_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_output_dir = Path(\"../output/radio_national_podcasts/transcripts/os\")\n",
    "shutil.rmtree(str(os_output_dir)) if os_output_dir.exists() else None\n",
    "os_output_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_path in tqdm(transcript_manifest.audio_path):\n",
    "    before = time.time()\n",
    "    transcript = transcribe_mono_audio(audio_path)\n",
    "    after = time.time()\n",
    "\n",
    "    os_transcript_record = {\n",
    "        \"hypothesis\": \" \".join(transcript.transcript.tolist()),\n",
    "        \"elapsed_time\": after - before,\n",
    "        \"provider\": \"os\",\n",
    "    }\n",
    "    (os_output_dir / f\"{Path(audio_path).stem}.json\").write_text(\n",
    "        json.dumps(os_transcript_record)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP\n",
    "- Huge chunks of the transcript missing when using async methods?\n",
    "- Probably use telephony model instead\n",
    "- Consider using streams instead of batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech, storage\n",
    "\n",
    "project = \"hobby-358221\"\n",
    "bucket_name = \"blog-os-asr\"\n",
    "storage_client = storage.Client(project=project)\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blobs = bucket.list_blobs()\n",
    "gcp_uris = [f\"gs://{bucket_name}/{e.name}\" for e in blobs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "\n",
    "def transcribe_gcs(gcs_uri):\n",
    "    \"\"\"Asynchronously transcribes the audio file specified by the gcs_uri.\"\"\"\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    operation = speech_client.long_running_recognize(config=config, audio=audio)\n",
    "    res = operation.result()\n",
    "    return \" \".join([e.alternatives[0].transcript.strip() for e in res.results]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_output_dir = Path(\"../output/radio_national_podcasts/transcripts/gcp\")\n",
    "shutil.rmtree(str(gcp_output_dir)) if gcp_output_dir.exists() else None\n",
    "gcp_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for gcp_uri in tqdm(gcp_uris):\n",
    "    try:\n",
    "        print(f\"Transcribing {gcp_uri}...\")\n",
    "        before = time.time()\n",
    "        gcp_res = transcribe_gcs(gcp_uri)\n",
    "        after = time.time()\n",
    "        gcp_transcript_record = {\n",
    "            \"hypothesis\": gcp_res,\n",
    "            \"elapsed_time\": after - before,\n",
    "            \"provider\": \"gcp\",\n",
    "        }\n",
    "        stub_output = (gcp_output_dir / f\"{Path(gcp_uri).stem}.json\").write_text(\n",
    "            json.dumps(gcp_transcript_record)\n",
    "        )\n",
    "    except Exception:\n",
    "        print(f\"Unable to transcribe: {gcp_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially use CLI?\n",
    "gcloud ml speech recognize-long-running \\\n",
    "    'gs://blog-os-asr/test.wav' \\\n",
    "     --language-code='en-US' --async\n",
    "\n",
    "# poll result\n",
    "gcloud ml speech operations describe 1558607248830316847"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "aws_uris = fs.ls(\"blog-os-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from transcribe_util import CustomWaiter, WaitState\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TranscribeCompleteWaiter(CustomWaiter):\n",
    "    def __init__(self, client):\n",
    "        super().__init__(\n",
    "            \"TranscribeComplete\",\n",
    "            \"GetTranscriptionJob\",\n",
    "            \"TranscriptionJob.TranscriptionJobStatus\",\n",
    "            {\"COMPLETED\": WaitState.SUCCESS, \"FAILED\": WaitState.FAILURE},\n",
    "            client,\n",
    "        )\n",
    "\n",
    "    def wait(self, job_name):\n",
    "        self._wait(TranscriptionJobName=job_name)\n",
    "\n",
    "\n",
    "def start_job(\n",
    "    job_name,\n",
    "    media_uri,\n",
    "    media_format,\n",
    "    language_code,\n",
    "    transcribe_client,\n",
    "    vocabulary_name=None,\n",
    "):\n",
    "    try:\n",
    "        job_args = {\n",
    "            \"TranscriptionJobName\": job_name,\n",
    "            \"Media\": {\"MediaFileUri\": media_uri},\n",
    "            \"MediaFormat\": media_format,\n",
    "            \"LanguageCode\": language_code,\n",
    "        }\n",
    "        if vocabulary_name is not None:\n",
    "            job_args[\"Settings\"] = {\"VocabularyName\": vocabulary_name}\n",
    "        response = transcribe_client.start_transcription_job(**job_args)\n",
    "        job = response[\"TranscriptionJob\"]\n",
    "        logger.info(\"Started transcription job %s.\", job_name)\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't start transcription job %s.\", job_name)\n",
    "        raise\n",
    "    else:\n",
    "        return job\n",
    "\n",
    "\n",
    "def get_job(job_name, transcribe_client):\n",
    "    try:\n",
    "        response = transcribe_client.get_transcription_job(\n",
    "            TranscriptionJobName=job_name\n",
    "        )\n",
    "        job = response[\"TranscriptionJob\"]\n",
    "        logger.info(\"Got job %s.\", job[\"TranscriptionJobName\"])\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't get job %s.\", job_name)\n",
    "        raise\n",
    "    else:\n",
    "        return job\n",
    "\n",
    "\n",
    "def list_jobs(job_filter, transcribe_client):\n",
    "    try:\n",
    "        response = transcribe_client.list_transcription_jobs(JobNameContains=job_filter)\n",
    "        jobs = response[\"TranscriptionJobSummaries\"]\n",
    "        next_token = response.get(\"NextToken\")\n",
    "        while next_token is not None:\n",
    "            response = transcribe_client.list_transcription_jobs(\n",
    "                JobNameContains=job_filter, NextToken=next_token\n",
    "            )\n",
    "            jobs += response[\"TranscriptionJobSummaries\"]\n",
    "            next_token = response.get(\"NextToken\")\n",
    "        logger.info(\"Got %s jobs with filter %s.\", len(jobs), job_filter)\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't get jobs with filter %s.\", job_filter)\n",
    "        raise\n",
    "    else:\n",
    "        return jobs\n",
    "\n",
    "\n",
    "def delete_job(job_name, transcribe_client):\n",
    "    try:\n",
    "        transcribe_client.delete_transcription_job(TranscriptionJobName=job_name)\n",
    "        logger.info(\"Deleted job %s.\", job_name)\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't delete job %s.\", job_name)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_client = boto3.client(\"transcribe\")\n",
    "aws_output_dir = Path(\"../output/radio_national_podcasts/transcripts/aws\")\n",
    "shutil.rmtree(str(aws_output_dir)) if aws_output_dir.exists() else None\n",
    "aws_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for aws_uri in tqdm(aws_uris):\n",
    "    try:\n",
    "        print(f\"Transcribing {aws_uri}...\")\n",
    "        before = time.time()\n",
    "        job_name_simple = Path(aws_uri).name\n",
    "\n",
    "        # ensure a job doesn't already exist\n",
    "        for job in list_jobs(job_name_simple, transcribe_client):\n",
    "            delete_job(job[\"TranscriptionJobName\"], transcribe_client)\n",
    "\n",
    "        print(f\"Starting transcription job {job_name_simple}.\")\n",
    "        start_job(job_name_simple, f\"s3://{aws_uri}\", \"wav\", \"en-US\", transcribe_client)\n",
    "        transcribe_waiter = TranscribeCompleteWaiter(transcribe_client)\n",
    "        transcribe_waiter.wait(job_name_simple)\n",
    "        job_simple = get_job(job_name_simple, transcribe_client)\n",
    "        transcript_simple = requests.get(\n",
    "            job_simple[\"Transcript\"][\"TranscriptFileUri\"]\n",
    "        ).json()\n",
    "        after = time.time()\n",
    "        aws_transcript_record = {\n",
    "            \"hypothesis\": transcript_simple[\"results\"][\"transcripts\"][0][\"transcript\"],\n",
    "            \"elapsed_time\": after - before,\n",
    "            \"provider\": \"aws\",\n",
    "        }\n",
    "        stub_output = (aws_output_dir / f\"{Path(aws_uri).stem}.json\").write_text(\n",
    "            json.dumps(aws_transcript_record)\n",
    "        )\n",
    "        # clean-up jobs\n",
    "        for job in list_jobs(job_name_simple, transcribe_client):\n",
    "            delete_job(job[\"TranscriptionJobName\"], transcribe_client)\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Unable to transcribe: {aws_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch transcription instructions: https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/samples/batch/python\n",
    "- Generate swagger, download python package, install python package\n",
    "- Example code via: https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/samples/batch/python/python-client/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key = os.environ[\"azure_asr_key\"]\n",
    "service_region = os.environ[\"azure_asr_region\"]\n",
    "endpoint = os.environ[\"azure_asr_endpoint\"]\n",
    "\n",
    "blob_key = os.environ[\"azure_blob_key\"]\n",
    "blob_connection_string = os.environ[\"azure_blob_connection_string\"]\n",
    "blob_container_name = os.environ[\"azure_blob_container_name\"]\n",
    "storage_account_name = os.environ[\"azure_storage_account_name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "# Licensed under the MIT license. See LICENSE.md file in the project root for full license information.\n",
    "import swagger_client as cris_client\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\",\n",
    ")\n",
    "\n",
    "# Your subscription key and region for the speech service\n",
    "SUBSCRIPTION_KEY = speech_key\n",
    "SERVICE_REGION = service_region\n",
    "\n",
    "NAME = \"Simple transcription\"\n",
    "DESCRIPTION = \"Simple transcription description\"\n",
    "\n",
    "LOCALE = \"en-US\"\n",
    "# with a single request. At least 'read' and 'list' (rl) permissions are required.\n",
    "# RECORDINGS_CONTAINER_URI = \"https://blogosasr.blob.core.windows.net/blog-os-asr\"\n",
    "RECORDINGS_CONTAINER_URI = \"https://blogosasr.blob.core.windows.net/blog-os-asr?sp=rl&st=2022-09-05T21:46:42Z&se=2022-09-06T05:46:42Z&spr=https&sv=2021-06-08&sr=c&sig=5cKlVMcMexgvS2BEI%2BWiaeNT1apvATLRJfK3E4lhTvM%3D\"\n",
    "\n",
    "# Set model information when doing transcription with custom models\n",
    "MODEL_REFERENCE = None  # guid of a custom model\n",
    "\n",
    "\n",
    "def transcribe_from_single_blob(uri, properties):\n",
    "    \"\"\"\n",
    "    Transcribe a single audio file located at `uri` using the settings specified in `properties`\n",
    "    using the base model for the specified locale.\n",
    "    \"\"\"\n",
    "    return cris_client.Transcription(\n",
    "        display_name=NAME,\n",
    "        description=DESCRIPTION,\n",
    "        locale=LOCALE,\n",
    "        content_urls=[uri],\n",
    "        properties=properties,\n",
    "    )\n",
    "\n",
    "\n",
    "def transcribe_with_custom_model(api, uri, properties):\n",
    "    \"\"\"\n",
    "    Transcribe a single audio file located at `uri` using the settings specified in `properties`\n",
    "    using the base model for the specified locale.\n",
    "    \"\"\"\n",
    "    if MODEL_REFERENCE is None:\n",
    "        logging.error(\"Custom model ids must be set when using custom models\")\n",
    "        sys.exit()\n",
    "    model = api.get_model(MODEL_REFERENCE)\n",
    "    return cris_client.Transcription(\n",
    "        display_name=NAME,\n",
    "        description=DESCRIPTION,\n",
    "        locale=LOCALE,\n",
    "        content_urls=[uri],\n",
    "        model=model,\n",
    "        properties=properties,\n",
    "    )\n",
    "\n",
    "\n",
    "def transcribe_from_container(uri, properties):\n",
    "    \"\"\"\n",
    "    Transcribe all files in the container located at `uri` using the settings specified in `properties`\n",
    "    using the base model for the specified locale.\n",
    "    \"\"\"\n",
    "    return cris_client.Transcription(\n",
    "        display_name=NAME,\n",
    "        description=DESCRIPTION,\n",
    "        locale=LOCALE,\n",
    "        content_container_url=uri,\n",
    "        properties=properties,\n",
    "    )\n",
    "\n",
    "\n",
    "def _paginate(api, paginated_object):  # sourcery skip: raise-specific-error\n",
    "    \"\"\"\n",
    "    The autogenerated client does not support pagination. This function returns a generator over\n",
    "    all items of the array that the paginated object `paginated_object` is part of.\n",
    "    \"\"\"\n",
    "    yield from paginated_object.values\n",
    "    typename = type(paginated_object).__name__\n",
    "    auth_settings = [\"apiKeyHeader\", \"apiKeyQuery\"]\n",
    "    while paginated_object.next_link:\n",
    "        link = paginated_object.next_link[len(api.api_client.configuration.host) :]\n",
    "        paginated_object, status, headers = api.api_client.call_api(\n",
    "            link, \"GET\", response_type=typename, auth_settings=auth_settings\n",
    "        )\n",
    "\n",
    "        if status == 200:\n",
    "            yield from paginated_object.values\n",
    "        else:\n",
    "            raise Exception(f\"could not receive paginated data: status {status}\")\n",
    "\n",
    "\n",
    "def delete_all_transcriptions(api):\n",
    "    \"\"\"\n",
    "    Delete all transcriptions associated with your speech resource.\n",
    "    \"\"\"\n",
    "    logging.info(\"Deleting all existing completed transcriptions.\")\n",
    "\n",
    "    # get all transcriptions for the subscription\n",
    "    transcriptions = list(_paginate(api, api.get_transcriptions()))\n",
    "\n",
    "    # Delete all pre-existing completed transcriptions.\n",
    "    # If transcriptions are still running or not started, they will not be deleted.\n",
    "    for transcription in transcriptions:\n",
    "        transcription_id = transcription._self.split(\"/\")[-1]\n",
    "        logging.debug(f\"Deleting transcription with id {transcription_id}\")\n",
    "        try:\n",
    "            api.delete_transcription(transcription_id)\n",
    "        except cris_client.rest.ApiException as exc:\n",
    "            logging.error(f\"Could not delete transcription {transcription_id}: {exc}\")\n",
    "\n",
    "\n",
    "def transcribe_azure(blob_sas):\n",
    "    logging.info(\"Starting transcription client...\")\n",
    "    before = time.time()\n",
    "\n",
    "    # configure API key authorization: subscription_key\n",
    "    configuration = cris_client.Configuration()\n",
    "    configuration.api_key[\"Ocp-Apim-Subscription-Key\"] = SUBSCRIPTION_KEY\n",
    "    configuration.host = (\n",
    "        f\"https://{SERVICE_REGION}.api.cognitive.microsoft.com/speechtotext/v3.0\"\n",
    "    )\n",
    "\n",
    "    # create the client object and authenticate\n",
    "    client = cris_client.ApiClient(configuration)\n",
    "\n",
    "    # create an instance of the transcription api class\n",
    "    api = cris_client.CustomSpeechTranscriptionsApi(api_client=client)\n",
    "\n",
    "    properties = {\n",
    "        # \"punctuationMode\": \"DictatedAndAutomatic\",\n",
    "        # \"profanityFilterMode\": \"Masked\",\n",
    "        # \"wordLevelTimestampsEnabled\": True,\n",
    "        # \"diarizationEnabled\": True,\n",
    "        # \"destinationContainerUrl\": \"<SAS Uri with at least write (w) permissions for an Azure Storage blob container that results should be written to>\",\n",
    "        # \"timeToLive\": \"PT1H\"\n",
    "    }\n",
    "\n",
    "    transcription_definition = transcribe_from_single_blob(blob_sas, properties)\n",
    "\n",
    "    # Uncomment this block to transcribe all files from a container.\n",
    "    # transcription_definition = transcribe_from_container(\n",
    "    #     RECORDINGS_CONTAINER_URI, properties)\n",
    "\n",
    "    created_transcription, status, headers = api.create_transcription_with_http_info(\n",
    "        transcription=transcription_definition\n",
    "    )\n",
    "\n",
    "    # get the transcription Id from the location URI\n",
    "    transcription_id = headers[\"location\"].split(\"/\")[-1]\n",
    "\n",
    "    logging.info(\n",
    "        f\"Created new transcription with id '{transcription_id}' in region {SERVICE_REGION}\"\n",
    "    )\n",
    "\n",
    "    logging.info(\"Checking status.\")\n",
    "\n",
    "    completed = False\n",
    "    while not completed:\n",
    "        # wait for 5 seconds before refreshing the transcription status\n",
    "        time.sleep(5)\n",
    "\n",
    "        transcription = api.get_transcription(transcription_id)\n",
    "        logging.info(f\"Transcriptions status: {transcription.status}\")\n",
    "\n",
    "        if transcription.status in (\"Failed\", \"Succeeded\"):\n",
    "            completed = True\n",
    "\n",
    "        if transcription.status == \"Succeeded\":\n",
    "            pag_files = api.get_transcription_files(transcription_id)\n",
    "            for file_data in _paginate(api, pag_files):\n",
    "                if file_data.kind != \"Transcription\":\n",
    "                    continue\n",
    "\n",
    "                file_data.name\n",
    "                results_url = file_data.links.content_url\n",
    "                results = requests.get(results_url)\n",
    "                after = time.time()\n",
    "                return {\n",
    "                    \"hypothesis\": results.json()[\"combinedRecognizedPhrases\"][0][\n",
    "                        \"display\"\n",
    "                    ],\n",
    "                    \"provider\": \"azure\",\n",
    "                    \"elapsed_time\": after - before,\n",
    "                }\n",
    "        elif transcription.status == \"Failed\":\n",
    "            logging.info(\n",
    "                f\"Transcription failed: {transcription.properties.error.message}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import (\n",
    "    BlobClient,\n",
    "    BlobSasPermissions,\n",
    "    BlobServiceClient,\n",
    "    ContainerClient,\n",
    "    __version__,\n",
    "    generate_blob_sas,\n",
    ")\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "blob_client = blob_service_client.get_container_client(blob_container_name)\n",
    "\n",
    "azure_output_dir = Path(\"../output/radio_national_podcasts/transcripts/azure\")\n",
    "shutil.rmtree(str(azure_output_dir)) if azure_output_dir.exists() else None\n",
    "azure_output_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "for blob_file in tqdm(list(blob_client.list_blobs())):\n",
    "    print(f\"Transcribing {blob_file['name']}...\")\n",
    "\n",
    "    blob_sas = generate_blob_sas(\n",
    "        account_name=storage_account_name,\n",
    "        container_name=blob_container_name,\n",
    "        blob_name=blob_file[\"name\"],\n",
    "        account_key=blob_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(hours=6),\n",
    "    )\n",
    "\n",
    "    blob_sas_formatted = f\"https://{storage_account_name}.blob.core.windows.net/{blob_container_name}/{blob_file['name']}?{blob_sas}\"\n",
    "\n",
    "    azure_transcript_record = transcribe_azure(blob_sas_formatted)\n",
    "    stub_output = (\n",
    "        azure_output_dir / f\"{Path(blob_file['name']).stem}.json\"\n",
    "    ).write_text(json.dumps(azure_transcript_record))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_transcripts(transcript_dir):\n",
    "    records = []\n",
    "    for e in transcript_dir.rglob(\"*.json\"):\n",
    "        record = json.loads(e.read_text())\n",
    "        record[\"stem\"] = e.stem\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "aws = load_transcripts(Path(\"../output/radio_national_podcasts/transcripts/aws\"))\n",
    "azure = load_transcripts(Path(\"../output/radio_national_podcasts/transcripts/azure\"))\n",
    "gcp = load_transcripts(Path(\"../output/radio_national_podcasts/transcripts/gcp\"))\n",
    "os = load_transcripts(Path(\"../output/radio_national_podcasts/transcripts/os\"))\n",
    "\n",
    "ground_truth = transcript_manifest.pipe(lambda x: x[[\"transcript\", \"stem\"]])\n",
    "\n",
    "wer_frames = []\n",
    "for provider in [aws, azure, gcp, os]:\n",
    "    wer_frames.append(\n",
    "        pd.merge(ground_truth, provider, how=\"inner\", on=\"stem\").assign(\n",
    "            wer=lambda x: x.apply(lambda y: wer(y.transcript, y.hypothesis), axis=1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "eval_res = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            e[[\"elapsed_time\", \"wer\"]].describe().assign(provider=e.iloc[0].provider)\n",
    "            for e in wer_frames\n",
    "        ]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .pipe(lambda x: x[x[\"index\"].str.contains(\"mean|min|50%|max\")])\n",
    "    .rename(mapper={\"index\": \"metric\"}, axis=\"columns\", inplace=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.barplot(x=\"provider\", y=\"elapsed_time\", data=eval_res)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ASR Provider x Elapsed Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "sns.barplot(x=\"provider\", y=\"wer\", hue=\"metric\", data=eval_res)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ASR Provider x WER (Various)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy up\n",
    "- keep two shortest podcasts under 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_records = (\n",
    "    transcript_manifest.query(\"len_minutes < 10\").sort_values(\"len_minutes\").head(2)\n",
    ")\n",
    "keep_records.to_csv('../output/radio_national_podcasts/manifest.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in Path(\"../output\").rglob(\"*\"):\n",
    "    if (\n",
    "        e.suffix in [\".json\", \".wav\", \".mp3\", \".DS_Store\", \".txt\"]\n",
    "        and e.stem not in keep_records.stem.tolist()\n",
    "    ):\n",
    "        e.unlink()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb0121aec9e40b71ec9730e04f00957539fc5aa06febb00ef12b9b6cf43c877e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
