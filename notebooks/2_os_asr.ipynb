{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)\n",
    "asr_logger = logging.getLogger(\"asr\")\n",
    "asr_logger.setLevel(logging.INFO)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model spot-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import nvsmi\n",
    "import torch\n",
    "\n",
    "# scope the models\n",
    "target_models = [\n",
    "    \"stt_en_quartznet15x5\",\n",
    "    \"stt_en_citrinet_512\",\n",
    "    \"stt_en_contextnet_512\",\n",
    "    \"stt_en_conformer_ctc_medium\",\n",
    "    \"stt_en_conformer_transducer_medium\",\n",
    "]\n",
    "pretrained_models = [\n",
    "    e\n",
    "    for e in nemo_asr.models.ASRModel.list_available_models()\n",
    "    if e.pretrained_model_name in target_models\n",
    "]\n",
    "\n",
    "# take a sample podcast\n",
    "audio_segment = AudioSegment.from_wav(\n",
    "    \"/home/blog-os-asr/output/temp_dir/rewilding-the-scottish-highlands.wav\"\n",
    ")\n",
    "\n",
    "# progressively slice to gauge memory usage\n",
    "s2ms = 1000\n",
    "seconds_increment = 30\n",
    "slice_intervals = [\n",
    "    (0, e * s2ms)\n",
    "    for e in list(range(0, math.ceil(audio_segment.duration_seconds) + 30, 30))[1:]\n",
    "]\n",
    "\n",
    "memory_usage_records = []\n",
    "for pretrained_model in pretrained_models:\n",
    "    print(f\"Memory testing: {pretrained_model.pretrained_model_name}\")\n",
    "\n",
    "    # model classes defined alongside model names\n",
    "    model = pretrained_model.class_.from_pretrained(\n",
    "        model_name=pretrained_model.pretrained_model_name\n",
    "    )\n",
    "    model_memory_footprint = nvsmi.get_gpu_processes()[0].used_memory\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # files as input; save in tmp dir\n",
    "        for interval in slice_intervals:\n",
    "            try:\n",
    "                slice = audio_segment[interval[0] : interval[1]]\n",
    "                save_message = slice.export(\n",
    "                    Path(temp_dir) / \"memory_test_fragment.wav\", format=\"wav\"\n",
    "                )\n",
    "                before = time.time()\n",
    "                transcription = model.transcribe(\n",
    "                    paths2audio_files=[\n",
    "                        str(Path(temp_dir) / \"memory_test_fragment.wav\")\n",
    "                    ],\n",
    "                    batch_size=1,\n",
    "                )\n",
    "                after = time.time()\n",
    "\n",
    "                # collect some metrics\n",
    "                memory_usage_records.append(\n",
    "                    {\n",
    "                        \"model_name\": pretrained_model.pretrained_model_name,\n",
    "                        \"input_size\": slice.duration_seconds,\n",
    "                        \"transcript\": transcription,\n",
    "                        \"memory_usage\": nvsmi.get_gpu_processes()[0].used_memory,\n",
    "                        \"time_elapsed\": after - before,\n",
    "                    }\n",
    "                )\n",
    "            except:\n",
    "                # out-of-memory > move onto next model\n",
    "                print(\"CUDA out of memory; skipping remaining slice intervals\")\n",
    "                break\n",
    "\n",
    "    # clear for next model\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage = pd.DataFrame(memory_usage_records)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.barplot(x=\"input_size\", y=\"time_elapsed\", hue=\"model_name\", data=memory_usage)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Input Size x Elapsed Time\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WER evaluation\n",
    "- given limitation of conformer/transducer models, bisect audio for all models > whole transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_transcription_records = []\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    for pretrained_model in pretrained_models:\n",
    "        print(f\"Processing for {pretrained_model.pretrained_model_name}\")\n",
    "        model = pretrained_model.class_.from_pretrained(\n",
    "            model_name=pretrained_model.pretrained_model_name\n",
    "        )\n",
    "        # oh god, bisect all audio\n",
    "        first = audio_segment[\n",
    "            : math.ceil(audio_segment.duration_seconds / 2) * 1000\n",
    "        ].export(Path(temp_dir) / \"first.wav\", format=\"wav\")\n",
    "        second = audio_segment[\n",
    "            math.ceil(audio_segment.duration_seconds / 2) * 1000 :\n",
    "        ].export(Path(temp_dir) / \"second.wav\", format=\"wav\")\n",
    "        transcriptions = model.transcribe(\n",
    "            paths2audio_files=[\n",
    "                str(Path(temp_dir) / \"first.wav\"),\n",
    "                str(Path(temp_dir) / \"second.wav\"),\n",
    "            ],\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "        joined_transcriptions = (\n",
    "            \" \".join(transcriptions)\n",
    "            if type(transcriptions) == list\n",
    "            else \" \".join(transcriptions[0])\n",
    "        )\n",
    "        full_transcription_records.append(\n",
    "            {\n",
    "                \"model\": pretrained_model.pretrained_model_name,\n",
    "                \"transcript\": joined_transcriptions,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "ground_truth = Path(\n",
    "    \"/home/blog-os-asr/output/radio_national_podcasts/transcripts/rewilding-the-scottish-highlands.txt\"\n",
    ").read_text()\n",
    "\n",
    "full_transcriptions = pd.DataFrame(full_transcription_records).assign(\n",
    "    wer=lambda x: x.transcript.apply(lambda y: wer(ground_truth, y))\n",
    ")\n",
    "\n",
    "full_transcriptions.pipe(lambda x: x[[\"model\", \"wer\"]])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "DIA_MODEL_NAME = \"pyannote/speaker-diarization@2022.07\"\n",
    "DIA_MODEL = Pipeline.from_pretrained(DIA_MODEL_NAME)\n",
    "PAUSE_THRESHOLD = 1\n",
    "MS = 1000\n",
    "\n",
    "\n",
    "def diarize_mono_audio(in_file, audio_segment):\n",
    "    diarization_raw = DIA_MODEL(str(in_file))\n",
    "    diarized_segments = (\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n",
    "                for turn, _, speaker in diarization_raw.itertracks(yield_label=True)\n",
    "            ]\n",
    "        )\n",
    "        # shift speaker attribution > mark/collapse consecutive speaker segments\n",
    "        .assign(segment_marker=lambda x: x.speaker.shift(1))\n",
    "        .assign(segment_marker=lambda x: x.segment_marker != x.speaker)\n",
    "        .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker))\n",
    "        # groupby segment, merge audio start/end times\n",
    "        .groupby(\"segment_marker\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"speaker\": \"first\",\n",
    "                \"start\": \"first\",\n",
    "                \"end\": \"last\",\n",
    "                \"segment_marker\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .rename(\n",
    "            mapper={\"segment_marker\": \"segment_marker_count\"},\n",
    "            axis=\"columns\",\n",
    "            inplace=False,\n",
    "        )\n",
    "        .assign(segment_len=lambda x: x.end - x.start)\n",
    "        # TODO: finesse a merging strategy\n",
    "        .query(\"segment_len >= @PAUSE_THRESHOLD\")\n",
    "        .reset_index(drop=True)\n",
    "        .assign(\n",
    "            audio_segment=lambda x: x.apply(\n",
    "                lambda y: _assign_child_segment(y, audio_segment), axis=1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return diarized_segments\n",
    "\n",
    "\n",
    "def _assign_child_segment(record, parent_audio_segment):\n",
    "    return parent_audio_segment[record.start * MS : record.end * MS]\n",
    "\n",
    "\n",
    "in_file = \"/home/blog-os-asr/output/temp_dir/rewilding-the-scottish-highlands.wav\"\n",
    "audio_segment = AudioSegment.from_file(in_file)\n",
    "diarized_segments = diarize_mono_audio(in_file, audio_segment)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment, silence, utils\n",
    "\n",
    "ASR_LOGGER = logging.getLogger(\"asr\")\n",
    "ASR_LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "SECOND_MAX_AUDIO = 240\n",
    "\n",
    "\n",
    "def _pseudo_optimise_silence_split(audio_segment):\n",
    "    # note, silence splitting has effect of reducing broader segment > small amounts of drift\n",
    "    dbfs_min = 10\n",
    "    dbfs_max = 40\n",
    "    dbfs_delta = 10\n",
    "    min_silence_len = 500  # ms\n",
    "    dBFS = audio_segment.dBFS\n",
    "    audio_segments = silence.split_on_silence(\n",
    "        audio_segment, min_silence_len=min_silence_len, silence_thresh=dBFS - dbfs_min\n",
    "    )\n",
    "    while (\n",
    "        pd.Series([e.duration_seconds for e in audio_segments]).median()\n",
    "        >= SECOND_MAX_AUDIO\n",
    "        and dbfs_min <= dbfs_max\n",
    "    ):\n",
    "        ASR_LOGGER.warning(\n",
    "            f\"Unable to split segment on silences with silence_thresh of {dBFS - dbfs_min}; re-attempting..\"\n",
    "        )\n",
    "        dbfs_min += dbfs_delta\n",
    "        audio_segments = silence.split_on_silence(\n",
    "            audio_segment,\n",
    "            min_silence_len=min_silence_len,\n",
    "            silence_thresh=dBFS - dbfs_min,\n",
    "        )\n",
    "\n",
    "    return audio_segments\n",
    "\n",
    "\n",
    "def segment_utterances(audio_segment_record):\n",
    "    if audio_segment_record.segment_len > SECOND_MAX_AUDIO:\n",
    "        silence_splits = _pseudo_optimise_silence_split(\n",
    "            audio_segment_record.audio_segment\n",
    "        )\n",
    "        all_splits = []\n",
    "        for split in silence_splits:\n",
    "            if split.duration_seconds > SECOND_MAX_AUDIO:\n",
    "                all_splits.extend(utils.make_chunks(split, SECOND_MAX_AUDIO * MS))\n",
    "            else:\n",
    "                all_splits.append(split)\n",
    "\n",
    "        start_times = []\n",
    "        start_time = audio_segment_record.start\n",
    "        # no cumsum unfortunately\n",
    "        for e in all_splits:\n",
    "            start_times.append(start_time)\n",
    "            start_time += e.duration_seconds\n",
    "\n",
    "        segments = (\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"audio_segment\": e,\n",
    "                        \"speaker\": audio_segment_record.speaker,\n",
    "                        \"segment_len\": e.duration_seconds,\n",
    "                    }\n",
    "                    for e in all_splits\n",
    "                ]\n",
    "            )\n",
    "            .assign(start=start_times)\n",
    "            .assign(end=lambda x: x.start + x.segment_len)\n",
    "        )\n",
    "        return segments\n",
    "    else:\n",
    "        return audio_segment_record.to_frame().T\n",
    "\n",
    "\n",
    "chunked_diarized_segments = diarized_segments.apply(\n",
    "    lambda x: segment_utterances(x), axis=1\n",
    ")\n",
    "chunked_diarized_segments = pd.concat(chunked_diarized_segments.tolist()).reset_index(\n",
    "    drop=True\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ASR_MODEL_NAME = \"stt_en_quartznet15x5\"\n",
    "ASR_MODEL = nemo_asr.models.ASRModel.from_pretrained(model_name=ASR_MODEL_NAME)\n",
    "BATCH_SIZE = 4\n",
    "temp_dir = Path(\"../output/temp_dir\")\n",
    "\n",
    "paths2audio_files = []  # explicitly sequence, RE: sorted() issues\n",
    "for idx, record in chunked_diarized_segments.iterrows():\n",
    "    segment_audio_res = record.audio_segment.export(\n",
    "        Path(temp_dir) / f\"chunk_{idx}.wav\", format=\"wav\"\n",
    "    )\n",
    "    paths2audio_files.append(str(Path(temp_dir) / f\"chunk_{idx}.wav\"))\n",
    "\n",
    "asr_outputs = ASR_MODEL.transcribe(\n",
    "    paths2audio_files=paths2audio_files,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    return_hypotheses=True,\n",
    ")\n",
    "chunked_diarized_segments = chunked_diarized_segments.assign(asr_outputs=asr_outputs)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "\n",
    "PUNCT_MODEL_NAME = \"punctuation_en_bert\"\n",
    "PUNCT_MODEL = PunctuationCapitalizationModel.from_pretrained(PUNCT_MODEL_NAME)\n",
    "\n",
    "\n",
    "def _punctuate_collapse_segment(record):\n",
    "    return {\n",
    "        \"speaker\": record.iloc[0].speaker,\n",
    "        \"start\": record.start.min(),\n",
    "        \"end\": record.end.max(),\n",
    "        \"transcript\": PUNCT_MODEL.add_punctuation_capitalization(\n",
    "            [\" \".join(record.asr_outputs.apply(lambda x: x.text).tolist())]\n",
    "        )[0],\n",
    "    }\n",
    "\n",
    "\n",
    "punctuated_exchanges = pd.DataFrame(\n",
    "    chunked_diarized_segments.assign(segment_marker=lambda x: x.speaker.shift(1))\n",
    "    .assign(segment_marker=lambda x: x.segment_marker != x.speaker)\n",
    "    .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker))\n",
    "    .groupby(\"segment_marker\")\n",
    "    .apply(_punctuate_collapse_segment)\n",
    "    .tolist()\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 400)\n",
    "\n",
    "punctuated_exchanges\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test module invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from asr import transcribe_mono_audio\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "transcription = transcribe_mono_audio(\n",
    "    \"../output/radio_national_podcasts/audio/rewilding-the-scottish-highlands.mp3\"\n",
    ")\n",
    "transcription\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb0121aec9e40b71ec9730e04f00957539fc5aa06febb00ef12b9b6cf43c877e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
