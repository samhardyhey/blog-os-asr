{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from jiwer import wer\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP3 Conversion\n",
    "- Mainly for GCP input constraints, but also for consistency with os implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_normalise_audio(in_file, out_file, sample_rate=16000):\n",
    "    if not os.path.exists(in_file):\n",
    "        raise ValueError(f\"{in_file} not found\")\n",
    "    if out_file is None:\n",
    "        out_file = in_file.replace(os.path.splitext(\n",
    "            in_file)[-1], f\"_{sample_rate}.wav\")\n",
    "\n",
    "    os.system(\n",
    "        f\"ffmpeg -i {in_file} -acodec pcm_s16le -ac 1 -af aresample=resampler=soxr -ar {sample_rate} {out_file} -y\"\n",
    "    )\n",
    "    return out_file\n",
    "\n",
    "\n",
    "transcript_manifest = pd.read_csv(\n",
    "    '../output/radio_national_podcasts/manifest.csv')\n",
    "\n",
    "output_dir = Path(\"../output/radio_national_podcasts/audio/wav\")\n",
    "shutil.rmtree(str(output_dir)) if output_dir.exists() else None\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for idx, record in transcript_manifest.iterrows():\n",
    "    input_path = Path(record.audio_path)\n",
    "    output_path = input_path.parents[1] / f\"wav/{input_path.stem}.wav\"\n",
    "    resample_normalise_audio(str(input_path), str(output_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from asr import transcribe_mono_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_transcript_records = []\n",
    "for audio_path in tqdm(transcript_manifest.head(5).audio_path):\n",
    "    before = time.time()\n",
    "    transcript = transcribe_mono_audio(audio_path)\n",
    "    after = time.time()\n",
    "    os_transcript_records.append({'hypothesis': ' '.join(transcript.transcript.tolist()),\n",
    "    'elapsed_time': after - before,\n",
    "    'provider': 'os'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"this is an ABC podcast\" intro from hypothesis\n",
    "os_wer = (pd.concat([transcript_manifest.head(), pd.DataFrame(os_transcript_records)], axis=1)\n",
    ".assign(wer=lambda x: x.apply(lambda y: wer(y.transcript, y.hypothesis), axis=1))\n",
    ".pipe(lambda x: x[['transcript','hypothesis','stem','transcript_len','wer']])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP\n",
    "- Huge chunks of the transcript missing when using async methods?\n",
    "- Probably use telephony model instead\n",
    "- Consider using streams instead of batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import speech\n",
    "\n",
    "project = 'hobby-358221'\n",
    "bucket_name = 'blog-os-asr'\n",
    "storage_client = storage.Client(project=project)\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blobs = bucket.list_blobs()\n",
    "gcp_uris = [f\"gs://{bucket_name}/{e.name}\" for e in blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "def transcribe_gcs(gcs_uri):\n",
    "    \"\"\"Asynchronously transcribes the audio file specified by the gcs_uri.\"\"\"\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    operation = speech_client.long_running_recognize(config=config, audio=audio)\n",
    "    res = operation.result()\n",
    "    return ' '.join([e.alternatives[0].transcript.strip() for e in res.results]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_output_dir = Path(\"../output/radio_national_podcasts/transcripts/gcp\")\n",
    "shutil.rmtree(str(gcp_output_dir)) if gcp_output_dir.exists() else None\n",
    "gcp_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for gcp_uri in tqdm(gcp_uris):\n",
    "    try:\n",
    "        print(f\"Transcribing {gcp_uri}...\")\n",
    "        before = time.time()\n",
    "        gcp_res = transcribe_gcs(gcp_uri)\n",
    "        after = time.time()\n",
    "        gcp_transcript_record = {'hypothesis': gcp_res,\n",
    "                                    'elapsed_time': after - before,\n",
    "                                    'provider': 'gcp'}\n",
    "        stub_output = (gcp_output_dir /\n",
    "                        f\"{Path(gcp_uri).stem}.json\").write_text(json.dumps(gcp_transcript_record))\n",
    "    except Exception:\n",
    "        print(f\"Unable to transcribe: {gcp_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially use CLI?\n",
    "gcloud ml speech recognize-long-running \\\n",
    "    'gs://blog-os-asr/test.wav' \\\n",
    "     --language-code='en-US' --async\n",
    "\n",
    "# poll result\n",
    "gcloud ml speech operations describe 1558607248830316847"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()\n",
    "aws_uris = fs.ls('blog-os-asr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import requests\n",
    "from transcribe_util import CustomWaiter, WaitState\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TranscribeCompleteWaiter(CustomWaiter):\n",
    "    def __init__(self, client):\n",
    "        super().__init__(\n",
    "            'TranscribeComplete', 'GetTranscriptionJob',\n",
    "            'TranscriptionJob.TranscriptionJobStatus',\n",
    "            {'COMPLETED': WaitState.SUCCESS, 'FAILED': WaitState.FAILURE},\n",
    "            client)\n",
    "\n",
    "    def wait(self, job_name):\n",
    "        self._wait(TranscriptionJobName=job_name)\n",
    "\n",
    "\n",
    "def start_job(\n",
    "        job_name, media_uri, media_format, language_code, transcribe_client,\n",
    "        vocabulary_name=None):\n",
    "    try:\n",
    "        job_args = {\n",
    "            'TranscriptionJobName': job_name,\n",
    "            'Media': {'MediaFileUri': media_uri},\n",
    "            'MediaFormat': media_format,\n",
    "            'LanguageCode': language_code}\n",
    "        if vocabulary_name is not None:\n",
    "            job_args['Settings'] = {'VocabularyName': vocabulary_name}\n",
    "        response = transcribe_client.start_transcription_job(**job_args)\n",
    "        job = response['TranscriptionJob']\n",
    "        logger.info(\"Started transcription job %s.\", job_name)\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't start transcription job %s.\", job_name)\n",
    "        raise\n",
    "    else:\n",
    "        return job\n",
    "\n",
    "\n",
    "def get_job(job_name, transcribe_client):\n",
    "    try:\n",
    "        response = transcribe_client.get_transcription_job(\n",
    "            TranscriptionJobName=job_name)\n",
    "        job = response['TranscriptionJob']\n",
    "        logger.info(\"Got job %s.\", job['TranscriptionJobName'])\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't get job %s.\", job_name)\n",
    "        raise\n",
    "    else:\n",
    "        return job\n",
    "\n",
    "\n",
    "def list_jobs(job_filter, transcribe_client):\n",
    "    try:\n",
    "        response = transcribe_client.list_transcription_jobs(\n",
    "            JobNameContains=job_filter)\n",
    "        jobs = response['TranscriptionJobSummaries']\n",
    "        next_token = response.get('NextToken')\n",
    "        while next_token is not None:\n",
    "            response = transcribe_client.list_transcription_jobs(\n",
    "                JobNameContains=job_filter, NextToken=next_token)\n",
    "            jobs += response['TranscriptionJobSummaries']\n",
    "            next_token = response.get('NextToken')\n",
    "        logger.info(\"Got %s jobs with filter %s.\", len(jobs), job_filter)\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't get jobs with filter %s.\", job_filter)\n",
    "        raise\n",
    "    else:\n",
    "        return jobs\n",
    "\n",
    "\n",
    "def delete_job(job_name, transcribe_client):\n",
    "    try:\n",
    "        transcribe_client.delete_transcription_job(\n",
    "            TranscriptionJobName=job_name)\n",
    "        logger.info(\"Deleted job %s.\", job_name)\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't delete job %s.\", job_name)\n",
    "        raise\n",
    "\n",
    "\n",
    "transcribe_client = boto3.client('transcribe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_output_dir = Path(\"../output/radio_national_podcasts/transcripts/aws\")\n",
    "shutil.rmtree(str(aws_output_dir)) if aws_output_dir.exists() else None\n",
    "aws_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for aws_uri in tqdm(aws_uris):\n",
    "    try:\n",
    "        print(f\"Transcribing {aws_uri}...\")\n",
    "        before = time.time()\n",
    "\n",
    "        job_name_simple = Path(aws_uri).name\n",
    "        print(f\"Starting transcription job {job_name_simple}.\")\n",
    "        start_job(job_name_simple,\n",
    "                  f's3://{aws_uri}', 'wav', 'en-US', transcribe_client)\n",
    "        transcribe_waiter = TranscribeCompleteWaiter(transcribe_client)\n",
    "        transcribe_waiter.wait(job_name_simple)\n",
    "        job_simple = get_job(job_name_simple, transcribe_client)\n",
    "        transcript_simple = requests.get(\n",
    "            job_simple['Transcript']['TranscriptFileUri']).json()\n",
    "        after = time.time()\n",
    "        aws_transcript_record = {'hypothesis': transcript_simple['results']['transcripts'][0]['transcript'],\n",
    "                                 'elapsed_time': after - before,\n",
    "                                 'provider': 'aws'}\n",
    "        stub_output = (aws_output_dir /\n",
    "                       f\"{Path(aws_uri).stem}.json\").write_text(json.dumps(aws_transcript_record))\n",
    "        # clean-up jobs\n",
    "        for job in list_jobs('test.wav', transcribe_client):\n",
    "            delete_job(job['TranscriptionJobName'], transcribe_client)\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Unable to transcribe: {aws_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up jobs\n",
    "for job in list_jobs('test.wav', transcribe_client):\n",
    "    delete_job(job['TranscriptionJobName'], transcribe_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key = os.environ['azure_asr_key']\n",
    "service_region = os.environ['azure_asr_region']\n",
    "endpoint = os.environ['azure_asr_endpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def speech_recognise_continuous_from_file():\n",
    "#     speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "#     audio_config = speechsdk.audio.AudioConfig(filename=\"/Users/samhardyhey/Desktop/blog/blog-os-asr/output/radio_national_podcasts/audio/wav/sugar-label-shopping-habits.wav\")\n",
    "\n",
    "#     speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "#     done = False\n",
    "\n",
    "#     def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "#         \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "#         print('CLOSING on {}'.format(evt))\n",
    "#         nonlocal done\n",
    "#         done = True\n",
    "\n",
    "#     # Connect callbacks to the events fired by the speech recognizer\n",
    "#     speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "#     speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "#     speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "#     speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "#     speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "#     # stop continuous recognition on either session stopped or canceled events\n",
    "#     speech_recognizer.session_stopped.connect(stop_cb)\n",
    "#     speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "#     # Start continuous speech recognition\n",
    "#     speech_recognizer.start_continuous_recognition()\n",
    "#     while not done:\n",
    "#         time.sleep(.5)\n",
    "\n",
    "#     speech_recognizer.stop_continuous_recognition()\n",
    "#     return speech_recognition_result.text\n",
    "#     # </SpeechContinuousRecognitionWithFile>\n",
    "\n",
    "# res = speech_recognise_continuous_from_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def azure_transcribe_audio(file_path):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=azure_asr_key, region=azure_asr_region)\n",
    "\n",
    "    speech_config.speech_recognition_language = \"en-US\"\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=file_path)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_recognition_result = speech_recognizer.recognize_once()\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        return speech_recognition_result.text\n",
    "\n",
    "res = azure_transcribe_audio(\"/Users/samhardyhey/Desktop/blog/blog-os-asr/output/radio_national_podcasts/audio/wav/sugar-label-shopping-habits.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Food labelling and the amount of added sugar in foods is a hot topic at the moment. Food Standards Australia New Zealand are currently looking at ways to help people choose lower sugar Foods. But a study released on Friday has had a counterintuitive finding that putting warning labels about added sugar on packages didn't consistently make people less inclined to buy them. Well, to talk us through the findings and what they could mean is one of its authors, Miranda Blake, welcome, Miranda.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb0121aec9e40b71ec9730e04f00957539fc5aa06febb00ef12b9b6cf43c877e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
