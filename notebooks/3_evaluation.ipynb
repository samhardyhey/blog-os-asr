{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transcript_manifest = pd.read_csv('../output/radio_national_podcasts/manifest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-02 15:46:01 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-09-02 15:46:03 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-09-02 15:46:03 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "torchvision is not available - cannot save figures\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "from asr import transcribe_mono_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mp3, from '/home/blog-os-asr/output/radio_national_podcasts/audio/29-august-health-report.mp3':\n",
      "  Duration: 00:28:35.78, start: 0.023021, bitrate: 128 kb/s\n",
      "    Stream #0:0: Audio: mp3, 48000 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : LAME3.98r\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/tmp/tmp0enlhi2l/29-august-health-report.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.29.100\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 pcm_s16le\n",
      "size=   53617kB time=00:28:35.74 bitrate= 256.0kbits/s speed= 537x    \n",
      "video:0kB audio:53617kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000142%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/blog-os-asr/notebooks/3_evaluation.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://notebooksb.jarvislabs.ai/home/blog-os-asr/notebooks/3_evaluation.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m transcript_manifest\u001b[39m.\u001b[39;49mhead()\u001b[39m.\u001b[39;49maudio_path\u001b[39m.\u001b[39;49mapply(transcribe_mono_audio)\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4322'>4323</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4323'>4324</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4329'>4330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4330'>4331</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4331'>4332</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4430'>4431</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4431'>4432</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/series.py?line=4432'>4433</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1083'>1084</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1084'>1085</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1085'>1086</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1087'>1088</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1141'>1142</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m             values,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1146'>1147</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1148'>1149</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1149'>1150</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1150'>1151</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pandas/core/apply.py?line=1151'>1152</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/blog-os-asr/notebooks/../asr.py:183\u001b[0m, in \u001b[0;36mtranscribe_mono_audio\u001b[0;34m(input_file, single_speaker)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=179'>180</a>\u001b[0m ASR_LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSuccessfully resampled/converted input to WAV\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=181'>182</a>\u001b[0m \u001b[39m# 2.0 diarize input\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=182'>183</a>\u001b[0m diarized_segments \u001b[39m=\u001b[39m diarize_mono_audio(wav_path, audio_segment)\n\u001b[1;32m    <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=183'>184</a>\u001b[0m ASR_LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSuccessfully diarized input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=185'>186</a>\u001b[0m \u001b[39m# 3.0 further segment into model-appropriate sizes\u001b[39;00m\n",
      "File \u001b[0;32m~/blog-os-asr/notebooks/../asr.py:44\u001b[0m, in \u001b[0;36mdiarize_mono_audio\u001b[0;34m(in_file, audio_segment)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiarize_mono_audio\u001b[39m(in_file, audio_segment):\n\u001b[0;32m---> <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=43'>44</a>\u001b[0m     diarization_raw \u001b[39m=\u001b[39m DIA_MODEL(\u001b[39mstr\u001b[39;49m(in_file))\n\u001b[1;32m     <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=44'>45</a>\u001b[0m     diarized_segments \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=45'>46</a>\u001b[0m         pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m     <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=46'>47</a>\u001b[0m             [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=78'>79</a>\u001b[0m         )\n\u001b[1;32m     <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=79'>80</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///home/blog-os-asr/notebooks/../asr.py?line=80'>81</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m diarized_segments\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/core/pipeline.py:210\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, file, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/core/pipeline.py?line=206'>207</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpreprocessors\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/core/pipeline.py?line=207'>208</a>\u001b[0m     file \u001b[39m=\u001b[39m ProtocolFile(file, lazy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessors)\n\u001b[0;32m--> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/core/pipeline.py?line=209'>210</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(file, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py:452\u001b[0m, in \u001b[0;36mSpeakerDiarization.apply\u001b[0;34m(self, file, num_speakers, min_speakers, max_speakers, hook)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=448'>449</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=449'>450</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=451'>452</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_embeddings(\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=452'>453</a>\u001b[0m         file,\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=453'>454</a>\u001b[0m         binarized_segmentations,\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=454'>455</a>\u001b[0m         exclude_overlap\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_exclude_overlap,\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=455'>456</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=456'>457</a>\u001b[0m     hook(\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m, embeddings)\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=457'>458</a>\u001b[0m     \u001b[39m#   shape: (num_chunks, local_num_speakers, dimension)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py:315\u001b[0m, in \u001b[0;36mSpeakerDiarization.get_embeddings\u001b[0;34m(self, file, binary_segmentations, exclude_overlap)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=311'>312</a>\u001b[0m mask_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvstack(masks)\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=312'>313</a>\u001b[0m \u001b[39m# (batch_size, num_frames) torch.Tensor\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=314'>315</a>\u001b[0m embedding_batch: np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding(\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=315'>316</a>\u001b[0m     waveform_batch, masks\u001b[39m=\u001b[39;49mmask_batch\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=316'>317</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=317'>318</a>\u001b[0m \u001b[39m# (batch_size, dimension) np.ndarray\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_diarization.py?line=319'>320</a>\u001b[0m embedding_batches\u001b[39m.\u001b[39mappend(embedding_batch)\n",
      "File \u001b[0;32m/opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py:193\u001b[0m, in \u001b[0;36mSpeechBrainPretrainedSpeakerEmbedding.__call__\u001b[0;34m(self, waveforms, masks)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=188'>189</a>\u001b[0m wav_lens \u001b[39m=\u001b[39m wav_lens \u001b[39m/\u001b[39m max_len\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=189'>190</a>\u001b[0m wav_lens[too_short] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=191'>192</a>\u001b[0m embeddings \u001b[39m=\u001b[39m (\n\u001b[0;32m--> <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=192'>193</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier_\u001b[39m.\u001b[39;49mencode_batch(signals, wav_lens\u001b[39m=\u001b[39;49mwav_lens)\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=193'>194</a>\u001b[0m     \u001b[39m.\u001b[39;49msqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=194'>195</a>\u001b[0m     \u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=195'>196</a>\u001b[0m     \u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=196'>197</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=198'>199</a>\u001b[0m embeddings[too_short\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mNAN\n\u001b[1;32m    <a href='file:///opt/conda/envs/p38/lib/python3.8/site-packages/pyannote/audio/pipelines/speaker_verification.py?line=200'>201</a>\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcript_manifest.head().audio_path.apply(transcribe_mono_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = transcribe_mono_audio(\n",
    "    \"../output/radio_national_podcasts/audio/rewilding-the-scottish-highlands.mp3\"\n",
    ")\n",
    "transcription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0a3389de74b7ec3a6acaa3d6c3d81172f0da4390709f30c0434c73a0ff8c437"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
