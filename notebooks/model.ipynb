{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import tempfile\n",
    "from copy import deepcopy\n",
    "from math import ceil, floor\n",
    "from pathlib import Path\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)\n",
    "asr_logger = logging.getLogger(\"asr\")\n",
    "asr_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path=None\n",
    "use_gpu=True\n",
    "default_dia_model = \"pyannote/pyannote-audio\"\n",
    "default_asr_model = \"stt_en_conformer_ctc_small\"  #'QuartzNet15x5Base-En'\n",
    "default_punct_model = \"punctuation_en_bert\"\n",
    "device = 0 if use_gpu else -1\n",
    "model_path = None if not model_path else Path(model_path)\n",
    "\n",
    "pause_threshold = 1  # RE: collapsing diarised segments\n",
    "batch_size = 4\n",
    "offset = -0.18  # calibration offset for timestamps: 180 ms\n",
    "\n",
    "# load models\n",
    "vocab = asr_model.decoder.vocabulary\n",
    "vocab.append(\"_\")\n",
    "decoder = CTCBeamDecoder(\n",
    "    vocab,\n",
    "    beam_width=1,\n",
    "    blank_id=vocab.index(\"_\"),\n",
    "    log_probs_input=True,\n",
    ")\n",
    "time_stride = (\n",
    "    1 / asr_model.cfg.preprocessor.window_size\n",
    ")  # duration of model timesteps\n",
    "\n",
    "# from_disk(model_path)\n",
    "TIME_PAD = 1\n",
    "    # huge possible max audio if model is Quartznet; maximise where possible to limit segmentation transcription error\n",
    "second_max_audio = (\n",
    "    120\n",
    "    if default_asr_model == \"QuartzNet15x5Base-En\"\n",
    "    else 4\n",
    ")\n",
    "# **Formatting params\n",
    "round_value = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_disk(model_path):\n",
    "    # for quicker reloading/prototyping\n",
    "    #         dia_model = dia_model\n",
    "    #         asr_model = asr_model\n",
    "    #         punct_model = punct_model\n",
    "\n",
    "    # try and load diarization model, use default otherwise\n",
    "    if model_path and (model_path / \"dia_model/model.pyannote\").exists():\n",
    "        try:\n",
    "            # urgh, whatever\n",
    "            with open((model_path / \"dia_model/model.pyannote\"), \"rb\") as f:\n",
    "                dia_model = pickle.load(f)\n",
    "        except:\n",
    "            asr_logger.warning(\n",
    "                f\"Unable to load: {str(model_path)}, using default pyannoate instead\"\n",
    "            )\n",
    "            dia_model = torch.hub.load(default_dia_model, \"dia\")\n",
    "    else:\n",
    "        dia_model = torch.hub.load(default_dia_model, \"dia\")\n",
    "\n",
    "    # similarly for ASR model\n",
    "    if model_path and (model_path / \"asr_model\").exists():\n",
    "        try:\n",
    "            asr_model = nemo_asr.models.ASRModel.restore_from(\n",
    "                str(model_path / \"asr_model/model.nemo\")\n",
    "            )\n",
    "        except:\n",
    "            asr_logger.warning(\n",
    "                f\"Unable to load: {model_path}, using default pyannoate instead\"\n",
    "            )\n",
    "            asr_model = nemo_asr.models.ASRModel.from_pretrained(\n",
    "                model_name=default_asr_model\n",
    "            )\n",
    "    else:\n",
    "        asr_model = nemo_asr.models.ASRModel.from_pretrained(\n",
    "            model_name=default_asr_model\n",
    "        )\n",
    "\n",
    "    # similarly for punctuation\n",
    "    if model_path and (model_path / \"punct_model\").exists():\n",
    "        try:\n",
    "            punct_model = nemo_asr.models.ASRModel.restore_from(\n",
    "                str(model_path / \"punct_model/model.nemo\")\n",
    "            )\n",
    "        except:\n",
    "            asr_logger.warning(\n",
    "                f\"Unable to load: {model_path}, using default pyannoate instead\"\n",
    "            )\n",
    "            punct_model = PunctuationCapitalizationModel.from_pretrained(\n",
    "                default_punct_model\n",
    "            )\n",
    "    else:\n",
    "        punct_model = PunctuationCapitalizationModel.from_pretrained(\n",
    "            default_punct_model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample_normalize_audio(in_file, out_file, sample_rate=16000):\n",
    "    # upsample/normalize audio to 16khz WAV\n",
    "    # via https://github.com/NVIDIA/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb\n",
    "    if not os.path.exists(in_file):\n",
    "        raise ValueError(f\"{in_file} not found\")\n",
    "    if out_file is None:\n",
    "        out_file = in_file.replace(os.path.splitext(in_file)[-1], f\"_{sample_rate}.wav\")\n",
    "\n",
    "    os.system(\n",
    "        f\"ffmpeg -i {in_file} -acodec pcm_s16le -ac 1 -af aresample=resampler=soxr -ar {sample_rate} {out_file} -y\"\n",
    "    )\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_stereo_audio(in_file, out_dir):\n",
    "    # into left/right channel wavs\n",
    "    in_file = Path(in_file)\n",
    "    out_dir = Path(out_dir)\n",
    "    assert in_file.exists()\n",
    "    assert out_dir.exists()\n",
    "\n",
    "    # format output files\n",
    "    left_channel = out_dir / f\"{Path(in_file).stem}_left.wav\"\n",
    "    right_channel = out_dir / f\"{Path(in_file).stem}_right.wav\"\n",
    "\n",
    "    # split, export\n",
    "    audio_segment = AudioSegment.from_file(in_file)\n",
    "    monos = audio_segment.split_to_mono()\n",
    "    assert len(monos) == 2  # cap support at stereo audio\n",
    "    monos[0].export(left_channel, format=\"wav\")\n",
    "    monos[1].export(right_channel, format=\"wav\")\n",
    "\n",
    "    return left_channel, right_channel\n",
    "\n",
    "\n",
    "def _diarize_mono_audio(in_file, single_speaker=False):\n",
    "    # apply diarization to mono audio\n",
    "    diarization_raw = dia_model({\"audio\": str(in_file)})\n",
    "    speaker_tag_remap = {e: idx + 1 for idx, e in enumerate(string.ascii_lowercase)}\n",
    "    if single_speaker:\n",
    "        # split diarized segments on time delta (risky)\n",
    "        # eg. in the case of stereo-split audio; assume a single speaker\n",
    "        diarized_segments = (\n",
    "            # format diarization as a dataframe\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n",
    "                    for turn, _, speaker in diarization_raw.itertracks(yield_label=True)\n",
    "                ]\n",
    "            )\n",
    "            # shift, get time deltas between diarization markers\n",
    "            .assign(segment_marker=lambda x: x.start - x.end.shift(1))\n",
    "            # first shifted record results in NAN; re-assign as 0 (eg. no previous record => no delta to calculate)\n",
    "            .assign(segment_marker=lambda x: x.segment_marker.fillna(0.0))\n",
    "            # check if deltas exceed threshold\n",
    "            .assign(segment_marker1=lambda x: x.segment_marker >= pause_threshold)\n",
    "            # create aggregation handle\n",
    "            .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker1))\n",
    "        )\n",
    "    else:\n",
    "        # split diarized segments on predicted speaker attribution (less risky? - original intent of model)\n",
    "        # eg. in the case of a \"mashed\" single audio file\n",
    "        diarized_segments = (\n",
    "            # format diarization as a dataframe\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker}\n",
    "                    for turn, _, speaker in diarization_raw.itertracks(yield_label=True)\n",
    "                ]\n",
    "            )\n",
    "            # shift speaker attribution\n",
    "            .assign(segment_marker=lambda x: x.speaker.shift(1)).assign(\n",
    "                segment_marker=lambda x: x.segment_marker != x.speaker\n",
    "            )\n",
    "            # create aggregation handle\n",
    "            .assign(segment_marker=lambda x: pd.Series.cumsum(x.segment_marker))\n",
    "        )\n",
    "\n",
    "    diarized_segments = (\n",
    "        diarized_segments\n",
    "        # groupby/aggregate shifted, collapse consecutive speaker sequences\n",
    "        .groupby(\"segment_marker\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"speaker\": \"first\",\n",
    "                \"start\": \"first\",\n",
    "                \"end\": \"last\",\n",
    "                \"segment_marker\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .rename(\n",
    "            mapper={\"segment_marker\": \"segment_marker_count\"},\n",
    "            axis=\"columns\",\n",
    "            inplace=False,\n",
    "        )\n",
    "        .assign(segment_len=lambda x: x.end - x.start)\n",
    "        # reconcile very short segments with pre/proceeding segment? merging strategy?\n",
    "        #             .query('segment_len >= 1')\n",
    "        # remap speaker tags from letters to numbers\n",
    "        .pipe(lambda x: x[x.speaker.apply(lambda y: True if type(y) == str else False)])\n",
    "        .assign(\n",
    "            speaker=lambda x: x.speaker.apply(\n",
    "                lambda y: int(speaker_tag_remap.get(y.lower()))\n",
    "            )\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return diarized_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_word_timestamps(asr_output, chunk_offset):\n",
    "    preds = asr_output.y_sequence.tolist()  # some funky formatting\n",
    "    probs_seq = torch.FloatTensor([preds])  # some funky formatting\n",
    "    beam_result, beam_scores, timesteps, out_seq_len = decoder.decode(probs_seq)\n",
    "    lens = out_seq_len[0][0]\n",
    "    timesteps = timesteps[0][0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    if len(timesteps) == 0:\n",
    "        return result\n",
    "\n",
    "    start = (timesteps[0] - TIME_PAD) * time_stride + chunk_offset\n",
    "    end = (timesteps[0] + TIME_PAD * 2) * time_stride + chunk_offset\n",
    "\n",
    "    token_prev = vocab[int(beam_result[0][0][0])]\n",
    "    word = token_prev\n",
    "\n",
    "    for n in range(1, lens):\n",
    "        token = vocab[int(beam_result[0][0][n])]\n",
    "\n",
    "        if token[0] == \"#\":\n",
    "            # merge subwords\n",
    "            word = word + token[2:]\n",
    "\n",
    "        elif token[0] == \"-\" or token_prev[0] == \"-\":\n",
    "            word = word + token\n",
    "\n",
    "        else:\n",
    "            word = word.replace(\"▁\", \"\").replace(\"_\", \"\")  # remove weird token\n",
    "\n",
    "            result_word = {\n",
    "                \"startTime\": int(start) / 1000,\n",
    "                \"endTime\": int(end) / 1000,\n",
    "                \"word\": word,\n",
    "            }\n",
    "            result.append(result_word)\n",
    "\n",
    "            start = (timesteps[n] - TIME_PAD) * time_stride + chunk_offset\n",
    "            word = token\n",
    "\n",
    "        end = (timesteps[n] + TIME_PAD * 2) * time_stride + chunk_offset\n",
    "        token_prev = token\n",
    "\n",
    "    # add last word\n",
    "    word = word.replace(\"▁\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "    result_word = {\n",
    "        \"startTime\": int(start) / 1000,\n",
    "        \"endTime\": int(end) / 1000,\n",
    "        \"word\": word,\n",
    "    }\n",
    "    result.append(result_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gcp_format_single_utterance(time_formatted_words_single, channel_tag=None):\n",
    "    # GCP STT consistency etc. why are we standardising on this again?\n",
    "    string_formatted_word_stamps = []\n",
    "    for e in time_formatted_words_single:\n",
    "        temp = deepcopy(e)\n",
    "        temp[\"startTime\"] = f\"{e['startTime']}s\"\n",
    "        temp[\"endTime\"] = f\"{e['endTime']}s\"\n",
    "        string_formatted_word_stamps.append(temp)\n",
    "\n",
    "    return {\n",
    "        \"alternatives\": [\n",
    "            {\n",
    "                \"transcript\": \" \".join(e[\"word\"] for e in time_formatted_words_single),\n",
    "                \"words\": string_formatted_word_stamps,\n",
    "            }\n",
    "        ],\n",
    "        \"speakerTag\": time_formatted_words_single[0][\"speakerTag\"],\n",
    "        \"channelTag\": \"None\" if not channel_tag else channel_tag,\n",
    "        \"languageCode\": \"en\",\n",
    "    }\n",
    "\n",
    "\n",
    "def _gcp_format_aggregate_transcript(time_formatted_words_all):\n",
    "    # GCP STT consistency etc.\n",
    "    transcript_all = [\n",
    "        \" \".join(e[\"word\"] for e in segment_transcript)\n",
    "        for segment_transcript in time_formatted_words_all\n",
    "    ]\n",
    "\n",
    "    return \" \".join(transcript_all)\n",
    "\n",
    "\n",
    "def _gcp_format_channel_seperated_transcript_objects(\n",
    "    gcp_formatted_left_res, gcp_formatted_right_res\n",
    "):\n",
    "    # merge, sort individual left/right transcripts\n",
    "    merged_utterances = pd.concat(\n",
    "        [\n",
    "            format_utterances_df(gcp_formatted_left_res),\n",
    "            format_utterances_df(gcp_formatted_right_res),\n",
    "        ]\n",
    "    ).sort_values(by=[\"startTime\", \"endTime\"])\n",
    "\n",
    "    # use any/left metadata as base (should be the same file right?)\n",
    "    merged_metadata = {\n",
    "        k: v for k, v in gcp_formatted_left_res[\"metadata\"].items() if k != \"transcript\"\n",
    "    }\n",
    "    merged_transcript = \" \".join(merged_utterances.transcript.tolist())\n",
    "    merged_metadata[\"transcript\"] = merged_transcript\n",
    "\n",
    "    return {\n",
    "        \"metadata\": merged_metadata,\n",
    "        \"streaming_outputs\": (\n",
    "            merged_utterances.pipe(\n",
    "                lambda x: x[\n",
    "                    [\n",
    "                        \"alternatives\",\n",
    "                        \"speakerTag\",\n",
    "                        \"channelTag\",\n",
    "                        \"languageCode\",\n",
    "                    ]\n",
    "                ]\n",
    "            ).to_dict(orient=\"records\")\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _naively_segment_utterances(record):\n",
    "    # apply naive splitting\n",
    "    n_chunks = int((record.end - record.start) // second_max_audio) + 1\n",
    "    chunk_len = (record.end - record.start) / n_chunks\n",
    "\n",
    "    df_temp = pd.DataFrame([record] * n_chunks).reset_index(drop=True)\n",
    "    df_temp[\"start\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len * x.name, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp[\"end\"] = df_temp.apply(\n",
    "        lambda x: x.start + chunk_len, axis=1\n",
    "    )  # increase start time\n",
    "    df_temp.loc[\n",
    "        (n_chunks - 1), \"end\"\n",
    "    ] = (\n",
    "        record.end\n",
    "    )  # adjust end time to actual time (sanity correction in case rounding cuts of audio)\n",
    "    return df_temp.assign(segment_len=lambda x: x.end - x.start)\n",
    "\n",
    "\n",
    "def _segment_utterances(audio_segment, record):\n",
    "    dBFS = audio_segment.dBFS  # audio volume (silence level is relative to volume)\n",
    "    silences = detect_silence(\n",
    "        audio_segment, min_silence_len=500, silence_thresh=dBFS - 20\n",
    "    )  # 0.5 break, time in ms, silence_thresh 20 lower than audio volume\n",
    "\n",
    "    if len(silences) == 0:\n",
    "        # no silence detected, lower min_silence_len\n",
    "        silences = detect_silence(\n",
    "            audio_segment, min_silence_len=100, silence_thresh=dBFS - 20\n",
    "        )\n",
    "\n",
    "        if len(silences) == 0:\n",
    "            # if still no silences detected after lowering min_silence_len, split naively\n",
    "            return _naively_segment_utterances(record)\n",
    "\n",
    "    silences = [[(s[1] - s[0]), s[0] / 1000, s[1] / 1000] for s in silences]  # ms -> s\n",
    "\n",
    "    df_temp = pd.DataFrame(record).T.reset_index(drop=True)\n",
    "\n",
    "    # split on longest silence, in middle of silence so no info is lost\n",
    "    while (len(silences) > 0) & any(df_temp.segment_len > second_max_audio):\n",
    "        longest_silence = silences.pop(silences.index(max(silences)))\n",
    "        middle_silence = record.start + (\n",
    "            longest_silence[1] + (longest_silence[2] - longest_silence[1]) / 2\n",
    "        )\n",
    "\n",
    "        record_to_split = df_temp.query(\n",
    "            f\"start < {middle_silence} & end>{middle_silence} & segment_len > {second_max_audio}\"\n",
    "        )\n",
    "        df_temp = df_temp.drop(record_to_split.index)\n",
    "\n",
    "        split_utterances = pd.DataFrame(\n",
    "            [record_to_split.iloc[0], record_to_split.iloc[0]]\n",
    "        ).reset_index(drop=True)\n",
    "        split_utterances.loc[0, \"end\"] = middle_silence\n",
    "        split_utterances.loc[1, \"start\"] = middle_silence\n",
    "        df_temp = (\n",
    "            pd.concat([df_temp, split_utterances])\n",
    "            .reset_index(drop=True)\n",
    "            .assign(segment_len=lambda x: x.end - x.start)\n",
    "        )\n",
    "\n",
    "    if any(df_temp.segment_len > second_max_audio):\n",
    "        # if any segments are still too long, naively split them\n",
    "        final_df = [df_temp.query(f\"segment_len < {second_max_audio}\")]\n",
    "        records_to_split = df_temp.query(f\"segment_len > {second_max_audio}\")\n",
    "\n",
    "        for i, record in records_to_split.iterrows():\n",
    "            final_df.append(_naively_segment_utterances(record))\n",
    "        return pd.concat(final_df).reset_index(drop=True).sort_values(by=[\"start\"])\n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transcribe_mono(input_file, single_speaker=False):\n",
    "    # transcribe a mono wav file\n",
    "    input_file = Path(input_file)\n",
    "    asr_logger.info(f\"Transcribing: {input_file}..\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # 1.0 resample, convert to wav\n",
    "        wav_path = _resample_normalize_audio(\n",
    "            input_file, str(Path(temp_dir) / f\"{Path(input_file).stem}.wav\")\n",
    "        )\n",
    "        audio_segment = AudioSegment.from_file(wav_path)\n",
    "\n",
    "        # 2.0 diarize input, save diarised segments\n",
    "        diarized_segments = _diarize_mono_audio(wav_path, single_speaker)\n",
    "        paths2audio_files = []  # explicitly sequence, RE: sorted() issues\n",
    "\n",
    "        chunked_diarized_segments = []\n",
    "        for idx, record in diarized_segments.iterrows():\n",
    "            if record.segment_len > second_max_audio:\n",
    "                records = _segment_utterances(\n",
    "                    audio_segment[floor(record.start * 1000) : ceil(record.end * 1000)],\n",
    "                    record,\n",
    "                )\n",
    "                chunked_diarized_segments.append(records)\n",
    "            else:\n",
    "                chunked_diarized_segments.append(\n",
    "                    pd.DataFrame(record).T.reset_index(drop=True)\n",
    "                )\n",
    "        chunked_diarized_segments = pd.concat(chunked_diarized_segments).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        for idx, record in chunked_diarized_segments.iterrows():\n",
    "            # slice audio per utterance, round start/end to floor/ceil inclusively\n",
    "            segment_audio = audio_segment[\n",
    "                floor(record.start * 1000) : ceil(record.end * 1000)\n",
    "            ]\n",
    "\n",
    "            # prevent misc output from printing\n",
    "            segment_audio_res = segment_audio.export(\n",
    "                Path(temp_dir) / f\"chunk_{idx}.wav\", format=\"wav\"\n",
    "            )\n",
    "            # collect segment audio path\n",
    "            paths2audio_files.append(str(Path(temp_dir) / f\"chunk_{idx}.wav\"))\n",
    "\n",
    "        # 3.0 batch transcribe, retrieve transcripts, alignments and logprobs for each utterance\n",
    "        outputs = asr_model.transcribe(\n",
    "            paths2audio_files=paths2audio_files,\n",
    "            batch_size=batch_size,\n",
    "            return_hypotheses=True,\n",
    "        )\n",
    "\n",
    "        # 4.0 retrieve/format timestamps\n",
    "        time_formatted_words_all = []\n",
    "        for idx, record in chunked_diarized_segments.iterrows():\n",
    "            time_formatted_words = _format_word_timestamps(\n",
    "                outputs[idx], record.start\n",
    "            )\n",
    "\n",
    "            # 5.0 apply punctuation to each output\n",
    "            punctuated_sequence = punct_model.add_punctuation_capitalization(\n",
    "                [\" \".join(e[\"word\"] for e in time_formatted_words)]\n",
    "            )[0]\n",
    "\n",
    "            if len(punctuated_sequence.split(\" \")) == len(time_formatted_words):\n",
    "                # easy case, where punctuated output len matches input len; assign directly\n",
    "                punctuated_sequence_joined = (\n",
    "                    pd.DataFrame(time_formatted_words)\n",
    "                    .assign(word=punctuated_sequence.split(\" \"))\n",
    "                    .assign(speakerTag=record.speaker)\n",
    "                    .to_dict(orient=\"records\")\n",
    "                )\n",
    "                time_formatted_words_all.append(punctuated_sequence_joined)\n",
    "            else:\n",
    "                # otherwise.. pad the difference? changes should be limited to immediately proceeding fullstops, commas, question marks\n",
    "                # https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html\n",
    "                print(\"Punctuated outputs not the same length as input\")\n",
    "\n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"source_file\": Path(input_file).name,\n",
    "                \"transcript\": _gcp_format_aggregate_transcript(\n",
    "                    time_formatted_words_all\n",
    "                ),\n",
    "                \"duration_seconds\": round(\n",
    "                    audio_segment.duration_seconds, round_value\n",
    "                ),\n",
    "            },\n",
    "            \"streaming_outputs\": [\n",
    "                _gcp_format_single_utterance(e) for e in time_formatted_words_all\n",
    "            ],\n",
    "        }\n",
    "\n",
    "\n",
    "def _transcribe_channel_seperated_audio(input_file):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # 1.0 split left/right channels\n",
    "        left_channel, right_channel = _split_stereo_audio(input_file, temp_dir)\n",
    "\n",
    "        # 2.0 process as seperate monos\n",
    "        left_res = _transcribe_mono(left_channel, single_speaker=True)\n",
    "        right_res = _transcribe_mono(right_channel, single_speaker=True)\n",
    "\n",
    "    # 3.0 merge outputs\n",
    "    return _gcp_format_channel_seperated_transcript_objects(left_res, right_res)\n",
    "\n",
    "\n",
    "def predict_single(input_file):\n",
    "    pass\n",
    "\n",
    "\n",
    "def predict_batch(input_files):\n",
    "    return [predict_single(e) for e in input_files]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
